{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b910b2be-7547-436c-ab39-6f932b6f0524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 03:17:55.409333: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-12 03:17:56.144772: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-12 03:17:56.304202: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-10-12 03:17:56.304256: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-10-12 03:17:58.964329: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-12 03:17:58.964985: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-10-12 03:17:58.965005: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/siam/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "import os\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, BatchNormalization\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51bc7f1-cafe-4fa7-88b3-4bc3c7dc8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23d3b3da-8f0d-4925-9159-1cdb5cb99029",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = [\n",
    "    '/mnt/c/Users/Siam/OneDrive - Tuskegee University/ai-arni-nsf/SAMPLE_dataset_public/png_images/qpm/real',\n",
    "    '/mnt/c/Users/Siam/OneDrive - Tuskegee University/ai-arni-nsf/SAMPLE_dataset_public/png_images/qpm/synth',\n",
    "    '/mnt/c/Users/Siam/OneDrive - Tuskegee University/ai-arni-nsf/SAMPLE_dataset_public/png_images/decibel/real',\n",
    "    '/mnt/c/Users/Siam/OneDrive - Tuskegee University/ai-arni-nsf/SAMPLE_dataset_public/png_images/decibel/synth'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497da62c-e7aa-4d71-b595-2eea8ec55dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_names = [\n",
    "    'cnn',\n",
    "    'resnet50',\n",
    "    'resnet101',\n",
    "    'resnet152',\n",
    "    'efficientnetb0',\n",
    "    'vgg16',\n",
    "    'vgg19',\n",
    "    'inceptionv3',\n",
    "    'unet'\n",
    "]\n",
    "\n",
    "classifiers = ['random_forest', 'svm', 'gradient_boosting', 'xgboost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d1ddaa-ba9d-46c6-9df9-7af8ca98c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "def set_random_seeds(seed=RANDOM_SEED):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "101481fa-d4f9-4e2b-a649-b68eb1346397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Data with Color Mode Option\n",
    "def load_data_from_directory(data_dir, img_size=(64, 64), color_mode='grayscale'):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    class_folders = os.listdir(data_dir)\n",
    "    for class_index, class_folder in enumerate(class_folders):\n",
    "        class_path = os.path.join(data_dir, class_folder)\n",
    "        if os.path.isdir(class_path):\n",
    "            for img_file in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                image = load_img(img_path, target_size=img_size, color_mode=color_mode)\n",
    "                image = img_to_array(image) / 255.0 # Normalised to [0, 1]\n",
    "                images.append(image)\n",
    "                labels.append(class_index)\n",
    "    \n",
    "    print(f'> {len(images)} files loaded of size {images[0].shape} with {len(labels)} labels')\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7939e754-7e48-4056-adaa-9053eaf9b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Augmentation with diverse transformations (Rotation, Brightness, Scaling, Translation, etc.)\n",
    "def augment_image(image, rotation_angle):\n",
    "    \"\"\"Apply a diverse set of augmentations to the image.\"\"\"\n",
    "    '''\n",
    "    image = tf.image.random_flip_left_right(image)  # Flip horizontally\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)  # Brightness adjustment\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)  # Contrast adjustment\n",
    "    image = tf.image.random_zoom(image, (0.8, 1.2))  # Zooming simulation\n",
    "    image = tf.image.random_translation(image, translations=[5, 5])  # Random Translation\n",
    "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)  # Saturation adjustment\n",
    "    image = tf.image.random_hue(image, max_delta=0.2)  # Hue adjustment\n",
    "    image = tf.image.random_jpeg_quality(image, min_jpeg_quality=50, max_jpeg_quality=100)  # JPEG quality jitter\n",
    "    image = tf.image.resize_with_crop_or_pad(image, 70, 70)  # Zooming simulation via resizing\n",
    "    image = tf.image.random_crop(image, size=[64, 64, 1])  # Crop back to original size\n",
    "    '''\n",
    "    # Apply rotation (keeping the original logic)\n",
    "    # rotation_angle = np.random.choice([0, 90, 180, 270])\n",
    "    if rotation_angle == 90:\n",
    "        image = tf.image.rot90(image)\n",
    "    elif rotation_angle == 180:\n",
    "        image = tf.image.rot90(image, k=2)\n",
    "    elif rotation_angle == 270:\n",
    "        image = tf.image.rot90(image, k=3)\n",
    "\n",
    "    label = rotation_angle // 90\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae042100-7e29-40ae-97c7-c2bbc472cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(images):\n",
    "    augmented_images = []\n",
    "    labels = []\n",
    "    for image in images:\n",
    "        for rotation_angle in [0, 90, 180, 270]:\n",
    "            aug_image, label = augment_image(image, rotation_angle)\n",
    "            augmented_images.append(aug_image)\n",
    "            labels.append(label)\n",
    "    print(f'> {len(augmented_images)} augmented images generated each of shape {augmented_images[0].shape} with {len(labels)} labels')\n",
    "    return np.array(augmented_images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be7eb411-78b1-4516-9df8-367185e7c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Balanced Sampling using Oversampling (Copies data from minor class to balance number of samples)\n",
    "def balance_classes(x_data, y_data):\n",
    "    ros = RandomOverSampler(random_state=RANDOM_SEED)\n",
    "    x_flattened = x_data.reshape((x_data.shape[0], -1))  # Reshape to 2D for balancing\n",
    "    x_resampled, y_resampled = ros.fit_resample(x_flattened, y_data)\n",
    "    x_resampled = x_resampled.reshape((-1, x_data.shape[1], x_data.shape[2], x_data.shape[3]))  # Reshape back to original\n",
    "    print(f'> (balancing) resampled to {x_resampled.shape[0]} samples')\n",
    "    return x_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb6222dd-c77a-47bd-afc2-0432a81652a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to build U-Net (custom   implementation)\n",
    "def build_unet_model(input_shape=(64, 64, 3)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoding (down-sampling) path\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "    # Bridge (bottom of the U-Net)\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    \n",
    "    # Decoding (up-sampling) path\n",
    "    u1 = layers.UpSampling2D((2, 2))(c3)\n",
    "    u1 = layers.Concatenate()([u1, c2])\n",
    "    \n",
    "    u2 = layers.UpSampling2D((2, 2))(u1)\n",
    "    u2 = layers.Concatenate()([u2, c1])\n",
    "    \n",
    "    outputs = layers.Conv2D(3, (1, 1), activation='softmax')(u2)\n",
    "    \n",
    "    return models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bef2de35-2232-48da-8108-9f9f72479677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make size variable\n",
    "def build_custom_cnn_model(input_shape=(64, 64, 1), num_classes=4, architecture_name=None, fine_tune=True):\n",
    "    print(f'> pretext task training: input_shape {input_shape}, num_classes {num_classes}, architecture_name {architecture_name}')\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    if architecture_name != 'cnn':\n",
    "        # Upsample input to the required size if needed (InceptionV3 requires minimum 75x75)\n",
    "        if architecture_name == 'inceptionv3' and (input_shape[0] < 75 or input_shape[1] < 75):\n",
    "            required_size = (75, 75)  # InceptionV3 minimum size\n",
    "        else:\n",
    "            required_size = (input_shape[0], input_shape[1])  # Default size for other models\n",
    "            \n",
    "        x = layers.Resizing(required_size[0], required_size[1])(inputs)  # Resize input to required size\n",
    "\n",
    "        # Convert grayscale (1-channel) to 3-channel RGB for pretrained models\n",
    "        x = layers.Conv2D(3, (1, 1))(x)\n",
    "        \n",
    "        # Pretrained model selection logic\n",
    "        # ResNet Variants\n",
    "        if architecture_name == 'resnet50':\n",
    "            from tensorflow.keras.applications import ResNet50\n",
    "            base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(required_size[0], required_size[1], 3))\n",
    "        elif architecture_name == 'resnet101':\n",
    "            from tensorflow.keras.applications import ResNet101\n",
    "            base_model = ResNet101(weights='imagenet', include_top=False, input_shape=(required_size[0], required_size[1], 3))\n",
    "        elif architecture_name == 'resnet152':\n",
    "            from tensorflow.keras.applications import ResNet152\n",
    "            base_model = ResNet152(weights='imagenet', include_top=False, input_shape=(required_size[0], required_size[1], 3))\n",
    "        \n",
    "        # EfficientNetB0\n",
    "        elif architecture_name == 'efficientnetb0':\n",
    "            from tensorflow.keras.applications import EfficientNetB0\n",
    "            base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(required_size[0], required_size[1], 3))\n",
    "\n",
    "        # VGGNet Variants\n",
    "        elif architecture_name == 'vgg16':\n",
    "            from tensorflow.keras.applications import VGG16\n",
    "            base_model = VGG16(weights='imagenet', include_top=False, input_shape=(required_size[0], required_size[1], 3))\n",
    "        elif architecture_name == 'vgg19':\n",
    "            from tensorflow.keras.applications import VGG19\n",
    "            base_model = VGG19(weights='imagenet', include_top=False, input_shape=(required_size[0], required_size[1], 3))\n",
    "\n",
    "        # InceptionV3\n",
    "        elif architecture_name == 'inceptionv3':\n",
    "            from tensorflow.keras.applications import InceptionV3\n",
    "            base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(required_size[0], required_size[1], 3))\n",
    "\n",
    "        # U-Net (not from Keras applications, custom U-Net function)\n",
    "        elif architecture_name == 'unet':\n",
    "            base_model = build_unet_model(input_shape=(required_size[0], required_size[1], 3))  # Custom function to build U-Net model\n",
    "\n",
    "        # Set base model to non-trainable if fine-tuning is disabled\n",
    "        if not fine_tune:\n",
    "            base_model.trainable = False\n",
    "        \n",
    "        # Apply base model to input\n",
    "        x = base_model(x, training=fine_tune)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    else:\n",
    "        # Build a custom CNN if no pretrained model is specified\n",
    "        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001))(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        x = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90be1973-ab59-4c08-92c2-5095232d613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building model: cnn\n",
      "> pretext task training: input_shape (128, 128, 1), num_classes 4, architecture_name cnn\n",
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_35 (InputLayer)       [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_220 (Conv2D)         (None, 128, 128, 32)      320       \n",
      "                                                                 \n",
      " batch_normalization_196 (Ba  (None, 128, 128, 32)     128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 64, 64, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_221 (Conv2D)         (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_197 (Ba  (None, 64, 64, 64)       256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 32, 32, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_222 (Conv2D)         (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_198 (Ba  (None, 32, 32, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 16, 16, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_223 (Conv2D)         (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_199 (Ba  (None, 16, 16, 256)      1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " global_average_pooling2d_18  (None, 256)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 523,396\n",
      "Trainable params: 522,436\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "\n",
      "Building model: resnet50\n",
      "> pretext task training: input_shape (128, 128, 1), num_classes 4, architecture_name resnet50\n",
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_36 (InputLayer)       [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " resizing_16 (Resizing)      (None, 128, 128, 1)       0         \n",
      "                                                                 \n",
      " conv2d_224 (Conv2D)         (None, 128, 128, 3)       6         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 4, 4, 2048)        23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d_19  (None, 2048)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 4)                 8196      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,595,914\n",
      "Trainable params: 23,542,794\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n",
      "\n",
      "Building model: resnet101\n",
      "> pretext task training: input_shape (128, 128, 1), num_classes 4, architecture_name resnet101\n",
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_38 (InputLayer)       [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " resizing_17 (Resizing)      (None, 128, 128, 1)       0         \n",
      "                                                                 \n",
      " conv2d_225 (Conv2D)         (None, 128, 128, 3)       6         \n",
      "                                                                 \n",
      " resnet101 (Functional)      (None, 4, 4, 2048)        42658176  \n",
      "                                                                 \n",
      " global_average_pooling2d_20  (None, 2048)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 4)                 8196      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,666,378\n",
      "Trainable params: 42,561,034\n",
      "Non-trainable params: 105,344\n",
      "_________________________________________________________________\n",
      "\n",
      "Building model: resnet152\n",
      "> pretext task training: input_shape (128, 128, 1), num_classes 4, architecture_name resnet152\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_40 (InputLayer)       [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " resizing_18 (Resizing)      (None, 128, 128, 1)       0         \n",
      "                                                                 \n",
      " conv2d_226 (Conv2D)         (None, 128, 128, 3)       6         \n",
      "                                                                 \n",
      " resnet152 (Functional)      (None, 4, 4, 2048)        58370944  \n",
      "                                                                 \n",
      " global_average_pooling2d_21  (None, 2048)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 4)                 8196      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,379,146\n",
      "Trainable params: 58,227,722\n",
      "Non-trainable params: 151,424\n",
      "_________________________________________________________________\n",
      "\n",
      "Building model: efficientnetb0\n",
      "> pretext task training: input_shape (128, 128, 1), num_classes 4, architecture_name efficientnetb0\n",
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_42 (InputLayer)       [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " resizing_19 (Resizing)      (None, 128, 128, 1)       0         \n",
      "                                                                 \n",
      " conv2d_227 (Conv2D)         (None, 128, 128, 3)       6         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, 4, 4, 1280)       4049571   \n",
      "                                                                 \n",
      " global_average_pooling2d_22  (None, 1280)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 4)                 5124      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,054,701\n",
      "Trainable params: 4,012,678\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n",
      "\n",
      "Building model: vgg16\n",
      "> pretext task training: input_shape (128, 128, 1), num_classes 4, architecture_name vgg16\n",
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_44 (InputLayer)       [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " resizing_20 (Resizing)      (None, 128, 128, 1)       0         \n",
      "                                                                 \n",
      " conv2d_228 (Conv2D)         (None, 128, 128, 3)       6         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_23  (None, 512)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,716,746\n",
      "Trainable params: 14,716,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Building model: vgg19\n",
      "> pretext task training: input_shape (128, 128, 1), num_classes 4, architecture_name vgg19\n",
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_46 (InputLayer)       [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " resizing_21 (Resizing)      (None, 128, 128, 1)       0         \n",
      "                                                                 \n",
      " conv2d_229 (Conv2D)         (None, 128, 128, 3)       6         \n",
      "                                                                 \n",
      " vgg19 (Functional)          (None, 4, 4, 512)         20024384  \n",
      "                                                                 \n",
      " global_average_pooling2d_24  (None, 512)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,026,442\n",
      "Trainable params: 20,026,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Building model: inceptionv3\n",
      "> pretext task training: input_shape (128, 128, 1), num_classes 4, architecture_name inceptionv3\n",
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_48 (InputLayer)       [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " resizing_22 (Resizing)      (None, 128, 128, 1)       0         \n",
      "                                                                 \n",
      " conv2d_230 (Conv2D)         (None, 128, 128, 3)       6         \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, 2, 2, 2048)        21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d_25  (None, 2048)             0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 4)                 8196      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,810,986\n",
      "Trainable params: 21,776,554\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n",
      "\n",
      "Building model: unet\n",
      "> pretext task training: input_shape (128, 128, 1), num_classes 4, architecture_name unet\n",
      "Model: \"model_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_50 (InputLayer)       [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " resizing_23 (Resizing)      (None, 128, 128, 1)       0         \n",
      "                                                                 \n",
      " conv2d_325 (Conv2D)         (None, 128, 128, 3)       6         \n",
      "                                                                 \n",
      " model_28 (Functional)       (None, 128, 128, 3)       372163    \n",
      "                                                                 \n",
      " global_average_pooling2d_26  (None, 3)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 4)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 372,185\n",
      "Trainable params: 372,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Loop through each model name and build the model\n",
    "for model_name in architecture_names:\n",
    "    print(f\"\\nBuilding model: {model_name}\")\n",
    "    model = build_custom_cnn_model(input_shape=(128, 128, 1), num_classes=4, architecture_name=model_name, fine_tune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ec10db-5f7c-4024-86f1-403908dd5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the trained model\n",
    "def save_model(model, architecture_name, data_dir):\n",
    "    model_name = f\"{architecture_name}_model_{os.path.basename(data_dir)}.h5\"\n",
    "    model.save(model_name)\n",
    "    print(f\"Model saved as {model_name}\")\n",
    "\n",
    "# Function to load the saved model\n",
    "def load_model(architecture_name, data_dir):\n",
    "    model_name = f\"{architecture_name}_model_{os.path.basename(data_dir)}.h5\"\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    print(f\"Model loaded from {model_name}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "043467f7-93a2-4d7f-ba18-d020d6f8e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Feature Extraction from deeper layers\n",
    "def extract_features(pretext_model, x_data, layer_index=-2):\n",
    "    intermediate_model = models.Model(inputs=pretext_model.input, outputs=pretext_model.layers[layer_index].output)\n",
    "    print('> intermediate feature extractor model:')\n",
    "    intermediate_model.summary()\n",
    "    features = intermediate_model.predict(x_data)\n",
    "    print(f'> extracted features of shape {features.shape}')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "507f84dc-f609-4cae-bc14-1b27627e0e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Downstream Task Training (RandomForest, SVM, Gradient Boosting, XGBoost)\n",
    "def train_downstream_task(train_features, train_labels, classifier='random_forest', n_splits=5):\n",
    "    print(f'> performing downstream task with {classifier}')\n",
    "    if classifier == 'random_forest':\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)\n",
    "    elif classifier == 'svm':\n",
    "        clf = SVC(kernel='linear', random_state=RANDOM_SEED)\n",
    "    elif classifier == 'gradient_boosting':\n",
    "        clf = GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_SEED)\n",
    "    elif classifier == 'xgboost':\n",
    "        clf = XGBClassifier(n_estimators=100, random_state=RANDOM_SEED, use_label_encoder=True\n",
    "        #  TODO clf = XGBClassifier(n_estimators=100, random_state=RANDOM_SEED, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "    \n",
    "    for train_index, test_index in skf.split(train_features, train_labels):\n",
    "        X_train, X_test = train_features[train_index], train_features[test_index]\n",
    "        y_train, y_test = train_labels[train_index], train_labels[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(precision_score(y_test, y_pred, average='macro'))\n",
    "        recalls.append(recall_score(y_test, y_pred, average='macro'))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    print(f'> [RESULT] Accuracy: {np.mean(accuracies) * 100:.2f}%')\n",
    "    print(f'> [RESULT] Precision: {np.mean(precisions) * 100:.2f}%')\n",
    "    print(f'> [RESULT] Recall: {np.mean(recalls) * 100:.2f}%')\n",
    "    print(f'> [RESULT] F1-score: {np.mean(f1_scores) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d0af66f-c01c-4d48-9082-fb4643fea8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Plot Results\n",
    "def plot_results(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14b10dfa-2e3a-4cfb-ab34-c234c6b763f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pretext_pipeline(data_dir, architecture_name='cnn'):\n",
    "    print(f'>>> BEGIN PRETEXT TASK\\n> folder: {data_dir}\\n> {architecture_name}')\n",
    "    # Load data\n",
    "    x_data, y_data = load_data_from_directory(data_dir)\n",
    "\n",
    "    # Augment Data\n",
    "    x_augmented, y_augmented = preprocess_data(x_data)\n",
    "\n",
    "    # Balance Data\n",
    "    x_balanced, y_balanced = balance_classes(x_augmented, y_augmented)\n",
    "\n",
    "    # Build and Train Pretext Task Model\n",
    "    pretext_model = build_custom_cnn_model(\n",
    "        input_shape=(64, 64, 1), \n",
    "        num_classes=4, \n",
    "        architecture_name=architecture_name, \n",
    "        fine_tune=False\n",
    "    )\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "    history = pretext_model.fit(x_balanced, y_balanced, epochs=10, validation_split=0.25, callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "    # Plot Results\n",
    "    plot_results(history)\n",
    "\n",
    "    # Save the trained model\n",
    "    save_model(pretext_model, architecture_name, data_dir)\n",
    "\n",
    "    print(f'> folder: {data_dir}\\n> {architecture_name}\\n>>> END PRETEXT TASK\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7569a4ee-3be2-4160-ae22-838f9cc49f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_downstream_pipeline(data_dir, architecture_name='cnn', downstream_classifier='random_forest', layer_index=-2):\n",
    "    print(f'>>> BEGIN DOWNSTREAM TASK\\n> folder: {data_dir}\\n> {architecture_name} | {downstream_classifier}')\n",
    "    \n",
    "    # Load data\n",
    "    x_data, y_data = load_data_from_directory(data_dir)\n",
    "\n",
    "    # Load the saved pretext model\n",
    "    pretext_model = load_model(architecture_name, data_dir)\n",
    "\n",
    "    # Extract Features\n",
    "    train_features = extract_features(pretext_model, x_data, layer_index=layer_index)\n",
    "\n",
    "    # Train the Downstream Task\n",
    "    train_downstream_task(train_features, y_data, classifier=downstream_classifier)\n",
    "\n",
    "    print(f'> folder: {data_dir}\\n> {architecture_name} | {downstream_classifier}\\n>>> END DOWNSTREAM TASK\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e760cf85-13e6-4c0c-ac3a-de262e2168a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> BEGIN PRETEXT TASK\n",
      "> folder: /mnt/c/Users/Siam/OneDrive - Tuskegee University/ai-arni-nsf/SAMPLE_dataset_public/png_images/qpm/real\n",
      "> cnn\n",
      "> 1345 files loaded of size (64, 64, 1) with 1345 labels\n",
      "> 5380 augmented images generated each of shape (64, 64, 1) with 5380 labels\n",
      "> (balancing) resampled to 5380 samples\n",
      "> pretext task training: input_shape (64, 64, 1), num_classes 4, architecture_name cnn\n",
      "Model: \"model_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_55 (InputLayer)       [(None, 64, 64, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_342 (Conv2D)         (None, 64, 64, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization_306 (Ba  (None, 64, 64, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_36 (MaxPoolin  (None, 32, 32, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_343 (Conv2D)         (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_307 (Ba  (None, 32, 32, 64)       256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_37 (MaxPoolin  (None, 16, 16, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_344 (Conv2D)         (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_308 (Ba  (None, 16, 16, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_38 (MaxPoolin  (None, 8, 8, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_345 (Conv2D)         (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_309 (Ba  (None, 8, 8, 256)        1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " global_average_pooling2d_30  (None, 256)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 523,396\n",
      "Trainable params: 522,436\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "127/127 [==============================] - 10s 65ms/step - loss: 0.4797 - accuracy: 0.9375 - val_loss: 3.8333 - val_accuracy: 0.2498 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "127/127 [==============================] - 7s 59ms/step - loss: 0.2323 - accuracy: 0.9970 - val_loss: 5.4269 - val_accuracy: 0.2498 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "127/127 [==============================] - 8s 61ms/step - loss: 0.1860 - accuracy: 0.9958 - val_loss: 15.2947 - val_accuracy: 0.2498 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "127/127 [==============================] - 8s 62ms/step - loss: 0.1379 - accuracy: 0.9985 - val_loss: 5.1778 - val_accuracy: 0.2989 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved as cnn_model_real.h5\n",
      "> folder: /mnt/c/Users/Siam/OneDrive - Tuskegee University/ai-arni-nsf/SAMPLE_dataset_public/png_images/qpm/real\n",
      "> cnn\n",
      ">>> END PRETEXT TASK\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, run all pretext tasks\n",
    "for data_dir in data_dirs[:1]:\n",
    "    for architecture_name in architecture_names[:1]:\n",
    "        run_pretext_pipeline(data_dir, architecture_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc33284-a525-4c62-9af2-9e19f229e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, (of course in different cell) run all downstream tasks\n",
    "for data_dir in data_dirs[:1]:\n",
    "    for architecture_name in architecture_names[:1]:\n",
    "        for classifier in classifiers[:1]:\n",
    "            run_downstream_pipeline(data_dir, architecture_name, classifier, -2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc696477-b9ce-47d3-98a0-f9ecc4d6659d",
   "metadata": {},
   "source": [
    "> performing downstream task with random_forest\n",
    "> [RESULT] Accuracy: 78.51%\n",
    "> [RESULT] Precision: 78.71%\n",
    "> [RESULT] Recall: 78.07%\n",
    "> [RESULT] F1-score: 78.02%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
