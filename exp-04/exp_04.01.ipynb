{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AH0eSlYf4Qtb",
    "outputId": "46355118-cc7c-4aa6-d240-d864b9eafaa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SAMPLE_dataset_public'...\n",
      "remote: Enumerating objects: 8159, done.\u001b[K\n",
      "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 8159 (delta 5), reused 13 (delta 3), pack-reused 8142 (from 1)\u001b[K\n",
      "Receiving objects: 100% (8159/8159), 1.41 GiB | 38.92 MiB/s, done.\n",
      "Resolving deltas: 100% (44/44), done.\n",
      "Updating files: 100% (8073/8073), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/benjaminlewis-afrl/SAMPLE_dataset_public.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xp1hRqDzyaGf"
   },
   "outputs": [],
   "source": [
    "# [Cell 1] - Import statements\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from tabulate import tabulate\n",
    "from typing import Dict, List, Any\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, BatchNormalization\n",
    "from datetime import timedelta\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPQ49kiKyaGi"
   },
   "outputs": [],
   "source": [
    "# [Cell 2] - Set global constants and configurations [config.py]\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "EXPERIMENT_BASE_DIR = 'exp-04'\n",
    "MODEL_DIR = os.path.join(EXPERIMENT_BASE_DIR, \"pretext_models\")\n",
    "DATA_DIR = os.path.join(EXPERIMENT_BASE_DIR, \"processed_data\")\n",
    "RESULTS_DIR = os.path.join(EXPERIMENT_BASE_DIR, \"results\")\n",
    "\n",
    "architecture_list = [\n",
    "    'cnn',\n",
    "    'resnet50',\n",
    "    'resnet101',\n",
    "    'resnet152',\n",
    "    'efficientnetb0',\n",
    "    'vgg16',\n",
    "    'vgg19',\n",
    "    'inceptionv3',\n",
    "    'unet'\n",
    "]\n",
    "\n",
    "classifier_list = [\n",
    "    'random_forest',\n",
    "    'svm',\n",
    "    'gradient_boosting',\n",
    "    'xgboost'\n",
    "]\n",
    "\n",
    "experiment_parameters = {\n",
    "    'mode': 'all',  # choices: 'process', 'pretext', 'downstream', 'all'\n",
    "    'split_index': 0,\n",
    "    'architecture': 'all',  # options: specific architecture like 'cnn' or 'all'\n",
    "    'classifier': 'all',  # choices: 'random_forest', 'svm', 'gradient_boosting', 'xgboost'\n",
    "    'data_dir': '/content/SAMPLE_dataset_public/png_images/qpm/real'\n",
    "}\n",
    "\n",
    "# Configuration dictionary\n",
    "hyperparameters = {\n",
    "    # Data loading and splitting\n",
    "    'img_size': (128, 128),\n",
    "    'color_mode': 'grayscale',\n",
    "    'test_split_size': 0.2,\n",
    "    'test_split_index': 0,\n",
    "\n",
    "    # Model architecture\n",
    "    'input_shape': (128, 128, 1),\n",
    "    'num_classes': 4,\n",
    "    'fine_tune': True,\n",
    "\n",
    "    # CNN architecture\n",
    "    'cnn_filters': [32, 64, 128, 256],\n",
    "    'cnn_kernel_size': (3, 3),\n",
    "    'cnn_pool_size': (2, 2),\n",
    "    'cnn_dense_units': 512,\n",
    "    'cnn_dropout_rate': 0.5,\n",
    "    'cnn_loss_function': 'sparse_categorical_crossentropy',\n",
    "    'cnn_activation_function': 'relu',\n",
    "    'cnn_padding': 'same',\n",
    "\n",
    "    # Other Pretext Architectures\n",
    "    'pretained_model_weights': 'imagenet',\n",
    "\n",
    "    # Training configuration\n",
    "    'batch_size': 32,\n",
    "    'epochs': 10,\n",
    "    'validation_split': 0.25,\n",
    "    'learning_rate': 0.001,\n",
    "    'early_stopping_patience': 3,\n",
    "    'lr_reduction_patience': 3,\n",
    "    'lr_reduction_factor': 0.5,\n",
    "\n",
    "    # Downstream task\n",
    "    'feature_extraction_layer': -2,\n",
    "    'rf_n_estimators': 100,\n",
    "    'svm_kernel': 'linear',\n",
    "    'gb_n_estimators': 100,\n",
    "    'xgb_n_estimators': 100,\n",
    "    'cv_splits': 5,\n",
    "}\n",
    "\n",
    "def display_configurations():\n",
    "    def create_section_table(keys, header):\n",
    "        table = [[key, hyperparameters[key]] for key in keys]\n",
    "        return f\"\\n{header}\\n\" + tabulate(table, headers=['Parameter', 'Value'], tablefmt='grid')\n",
    "\n",
    "    print(\"\\n========== Experiment Configurations ==========\")\n",
    "\n",
    "    # Architectures and Classifiers\n",
    "    print(\"\\n>>> Architectures <<<\")\n",
    "    print(tabulate([[\", \".join(architecture_list)]], tablefmt='grid'))\n",
    "\n",
    "    print(\"\\n>>> Classifiers <<<\")\n",
    "    print(tabulate([[\", \".join(classifier_list)]], tablefmt='grid'))\n",
    "\n",
    "    # Experiment Pipeline\n",
    "    print(\"\\n>>> THIS EXPERIMENT PIPELINE <<<\")\n",
    "    table = [[key, value] for key, value in experiment_parameters.items()]\n",
    "    print(tabulate(table, headers=['Parameter', 'Value'], tablefmt='grid'))\n",
    "\n",
    "    # Data Loading and Splitting\n",
    "    data_keys = ['img_size', 'color_mode', 'test_split_size', 'test_split_index']\n",
    "    print(create_section_table(data_keys, \">>> Data Loading and Splitting <<<\"))\n",
    "\n",
    "    # Model Architecture\n",
    "    model_keys = ['input_shape', 'num_classes', 'fine_tune']\n",
    "    print(create_section_table(model_keys, \">>> Model Architecture <<<\"))\n",
    "\n",
    "    # CNN Architecture\n",
    "    cnn_keys = ['cnn_filters', 'cnn_kernel_size', 'cnn_pool_size', 'cnn_dense_units',\n",
    "                'cnn_dropout_rate', 'cnn_loss_function', 'cnn_activation_function', 'cnn_padding']\n",
    "    print(create_section_table(cnn_keys, \">>> CNN Architecture <<<\"))\n",
    "\n",
    "    # Other Pretext Architectures\n",
    "    pretext_keys = ['pretained_model_weights']\n",
    "    print(create_section_table(pretext_keys, \">>> Other Pretext Architectures <<<\"))\n",
    "\n",
    "    # Training Configuration\n",
    "    training_keys = ['batch_size', 'epochs', 'validation_split', 'learning_rate',\n",
    "                    'early_stopping_patience', 'lr_reduction_patience', 'lr_reduction_factor']\n",
    "    print(create_section_table(training_keys, \">>> Training Configuration <<<\"))\n",
    "\n",
    "    # Downstream Task Configuration\n",
    "    downstream_keys = ['feature_extraction_layer', 'rf_n_estimators', 'svm_kernel',\n",
    "                      'gb_n_estimators', 'xgb_n_estimators', 'cv_splits']\n",
    "    print(create_section_table(downstream_keys, \">>> Downstream Task Configuration <<<\"))\n",
    "\n",
    "    print(\"\\n============================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ru6BTX4MyaGj"
   },
   "outputs": [],
   "source": [
    "# [Cell 3] - Utility functions [environment_checker.py/seeds.py/ResultsTracker.py]\n",
    "\n",
    "def check_environment():\n",
    "    \"\"\"Print and check library versions against requirements\"\"\"\n",
    "\n",
    "    # Import all required libraries\n",
    "    import sys\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sklearn\n",
    "    import xgboost as xgb\n",
    "    import matplotlib\n",
    "    from PIL import Image\n",
    "    import tabulate as tabulate_module  # Renamed to avoid confusion\n",
    "\n",
    "    # Define required versions\n",
    "    requirements = {\n",
    "        'tensorflow': '2.8.0',\n",
    "        'numpy': '1.19.2',\n",
    "        'scikit-learn': '0.24.2',\n",
    "        'matplotlib': '3.4.3',\n",
    "        'xgboost': '1.5.0',\n",
    "        'Pillow': '8.3.2',\n",
    "        'pandas': '2.2.3',\n",
    "        'tabulate': '0.9.0',\n",
    "        'ipython': '7.31.1',\n",
    "        'ipykernel': '6.29.5',\n",
    "        'ipywidgets': '8.1.5',\n",
    "        'jupyter_client': '8.6.3',\n",
    "        'jupyter_core': '5.7.2',\n",
    "        'jupyter_server': '2.14.2',\n",
    "        'jupyterlab': '4.2.5',\n",
    "        'nbclient': '0.10.0',\n",
    "        'nbconvert': '7.16.4',\n",
    "        'nbformat': '5.10.4',\n",
    "        'notebook': '7.2.2',\n",
    "        'traitlets': '5.14.3'\n",
    "    }\n",
    "\n",
    "    # Initialize table data\n",
    "    table_data = []\n",
    "\n",
    "    # Check core libraries with better error handling\n",
    "    try:\n",
    "        libraries_check = [\n",
    "            ('Python', sys.version.split()[0], 'N/A'),\n",
    "            ('TensorFlow', tf.__version__, requirements['tensorflow']),\n",
    "            ('NumPy', np.__version__, requirements['numpy']),\n",
    "            ('Pandas', pd.__version__, requirements['pandas']),\n",
    "            ('Scikit-learn', sklearn.__version__, requirements['scikit-learn']),\n",
    "            ('XGBoost', xgb.__version__, requirements['xgboost']),\n",
    "            ('Matplotlib', matplotlib.__version__, requirements['matplotlib']),\n",
    "            ('Pillow (PIL)', Image.__version__, requirements['Pillow']),\n",
    "        ]\n",
    "\n",
    "        # Add tabulate version separately with error handling\n",
    "        try:\n",
    "            tabulate_version = getattr(tabulate_module, '__version__', 'Version unknown')\n",
    "        except:\n",
    "            tabulate_version = 'Version unknown'\n",
    "        libraries_check.append(('Tabulate', tabulate_version, requirements['tabulate']))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking core library versions: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Add Jupyter-related libraries\n",
    "    jupyter_libs = {\n",
    "        'IPython': 'ipython',\n",
    "        'IPykernel': 'ipykernel',\n",
    "        'IPywidgets': 'ipywidgets',\n",
    "        'Jupyter Client': 'jupyter_client',\n",
    "        'Jupyter Core': 'jupyter_core',\n",
    "        'Jupyter Server': 'jupyter_server',\n",
    "        'JupyterLab': 'jupyterlab',\n",
    "        'NBClient': 'nbclient',\n",
    "        'NBConvert': 'nbconvert',\n",
    "        'NBFormat': 'nbformat',\n",
    "        'Notebook': 'notebook',\n",
    "        'Traitlets': 'traitlets'\n",
    "    }\n",
    "\n",
    "    for display_name, pkg_name in jupyter_libs.items():\n",
    "        try:\n",
    "            pkg = __import__(pkg_name)\n",
    "            version = getattr(pkg, '__version__', 'Version unknown')\n",
    "            req_version = requirements.get(pkg_name, 'N/A')\n",
    "            libraries_check.append((display_name, version, req_version))\n",
    "        except ImportError:\n",
    "            libraries_check.append((display_name, 'Not installed', requirements.get(pkg_name, 'N/A')))\n",
    "        except Exception as e:\n",
    "            libraries_check.append((display_name, f'Error: {str(e)}', requirements.get(pkg_name, 'N/A')))\n",
    "\n",
    "    # Create table with version comparison and status\n",
    "    for name, current, required in libraries_check:\n",
    "        if required == 'N/A':\n",
    "            status = '---'\n",
    "        elif current == 'Not installed' or 'Error' in str(current) or current == 'Version unknown':\n",
    "            status = '?'\n",
    "        else:\n",
    "            status = '✓' if current == required else '✗'\n",
    "\n",
    "        table_data.append([name, current, required, status])\n",
    "\n",
    "    # Print main version table\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"{' LIBRARY VERSIONS AND REQUIREMENTS ':=^100}\")\n",
    "    print(\"=\"*100)\n",
    "    print(tabulate_module.tabulate(table_data,\n",
    "                                 headers=['Library', 'Current Version', 'Required Version', 'Status'],\n",
    "                                 tablefmt='grid'))\n",
    "\n",
    "    # Print system and GPU information\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"{' SYSTEM INFORMATION ':=^100}\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    # System info\n",
    "    import platform\n",
    "    print(f\"OS: {platform.system()} {platform.version()}\")\n",
    "\n",
    "    # GPU info\n",
    "    print(\"\\nGPU Information:\")\n",
    "    print(f\"CUDA Available: {tf.test.is_built_with_cuda()}\")\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"Found {len(gpus)} GPU(s):\")\n",
    "        for gpu in gpus:\n",
    "            print(f\"  • {gpu}\")\n",
    "    else:\n",
    "        print(\"No GPUs found\")\n",
    "\n",
    "    # TensorFlow environment variables\n",
    "    tf_vars = [var for var in os.environ if 'TF_' in var]\n",
    "    if tf_vars:\n",
    "        print(\"\\nTensorFlow Environment Variables:\")\n",
    "        for var in tf_vars:\n",
    "            print(f\"  • {var}: {os.environ[var]}\")\n",
    "\n",
    "    print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "# def set_all_seeds(seed=42, gpu_deterministic=True):\n",
    "#     \"\"\"Set all seeds to make results reproducible\n",
    "#     Example:\n",
    "#         # For development/debugging (slower but reproducible)\n",
    "#         set_all_seeds(42, gpu_deterministic=True)\n",
    "\n",
    "#         # For production/training (faster but not fully reproducible)\n",
    "#         set_all_seeds(42, gpu_deterministic=False)\n",
    "#     \"\"\"\n",
    "#     # Basic seeds\n",
    "#     random.seed(seed)                            # Python random module\n",
    "#     np.random.seed(seed)                         # Numpy\n",
    "#     tf.random.set_seed(seed)                     # TensorFlow\n",
    "#     tf.keras.utils.set_random_seed(seed)         # Keras\n",
    "#     tf.experimental.numpy.random.seed(seed)      # TensorFlow numpy\n",
    "\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "#     if gpu_deterministic:\n",
    "#         # GPU-specific deterministic settings\n",
    "#         tf.config.experimental.enable_op_determinism()\n",
    "#         os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "#         os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "#         # Threading controls\n",
    "#         tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "#         tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "def set_all_seeds(seed=42, gpu_deterministic=False):  # Changed default to False\n",
    "    \"\"\"Set all seeds to make results reproducible\n",
    "    Args:\n",
    "        seed (int): The seed value to use\n",
    "        gpu_deterministic (bool): Whether to enable deterministic GPU operations\n",
    "            Set to False for better performance, True for reproducibility\n",
    "    \"\"\"\n",
    "    # Basic seeds\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    if gpu_deterministic:\n",
    "        print(\"> Warning: Enabling deterministic GPU operations may impact performance\")\n",
    "        # GPU-specific deterministic settings\n",
    "        tf.config.experimental.enable_op_determinism()\n",
    "        os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "        os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "        # Threading controls\n",
    "        tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "        tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    else:\n",
    "        print(\"> Using non-deterministic GPU operations for better performance\")\n",
    "        # Disable deterministic operations\n",
    "        if 'TF_DETERMINISTIC_OPS' in os.environ:\n",
    "            del os.environ['TF_DETERMINISTIC_OPS']\n",
    "        if 'TF_CUDNN_DETERMINISTIC' in os.environ:\n",
    "            del os.environ['TF_CUDNN_DETERMINISTIC']\n",
    "\n",
    "        try:\n",
    "            tf.config.experimental.disable_op_determinism()\n",
    "        except:\n",
    "            pass  # Ignore if the function doesn't exist in current TF version\n",
    "\n",
    "class ResultsTracker:\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "\n",
    "    def add_result(self, data_dir: str, architecture: str, classifier: str,\n",
    "                   cv_metrics: Dict[str, List[float]], test_metrics: Dict[str, Any]):\n",
    "        result = {\n",
    "            'Dataset': f\"{data_dir.split('/')[-2]}_{data_dir.split('/')[-1]}_test_split_ind_{hyperparameters['test_split_index']}\",\n",
    "            'Architecture': architecture,\n",
    "            'Classifier': classifier,\n",
    "            'CV_Accuracy': f\"{np.mean(cv_metrics['accuracies'])*100:.1f}±{np.std(cv_metrics['accuracies'])*100:.1f}\",\n",
    "            'CV_Precision': f\"{np.mean(cv_metrics['precisions'])*100:.1f}±{np.std(cv_metrics['precisions'])*100:.1f}\",\n",
    "            'CV_Recall': f\"{np.mean(cv_metrics['recalls'])*100:.1f}±{np.std(cv_metrics['recalls'])*100:.1f}\",\n",
    "            'CV_F1': f\"{np.mean(cv_metrics['f1_scores'])*100:.1f}±{np.std(cv_metrics['f1_scores'])*100:.1f}\",\n",
    "            'Test_Accuracy': f\"{test_metrics['accuracy']*100:.1f}\",\n",
    "            'Test_Precision': f\"{test_metrics['precision']*100:.1f}\",\n",
    "            'Test_Recall': f\"{test_metrics['recall']*100:.1f}\",\n",
    "            'Test_F1': f\"{test_metrics['f1']*100:.1f}\",\n",
    "            'Test_Confusion_Matrix': f\"{test_metrics['confusion_matrix']}\"\n",
    "        }\n",
    "        self.results.append(result)\n",
    "\n",
    "\n",
    "    def display_results(self):\n",
    "        if not self.results:\n",
    "            print(\"> No results to display\")\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(self.results)\n",
    "        grouped = df.groupby(['Dataset', 'Architecture'])\n",
    "\n",
    "        for (dataset, arch), group in grouped:\n",
    "            print(f\"\\n=== Results: Dataset: <{dataset}> | Pretext Model: <{arch}> ===\")\n",
    "\n",
    "            # Display test and cross-validation metrics\n",
    "            display_cols = [\n",
    "                'Classifier',\n",
    "                'Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1',\n",
    "                'CV_Accuracy', 'CV_Precision', 'CV_Recall', 'CV_F1'\n",
    "            ]\n",
    "\n",
    "            display_df = group[display_cols].copy()\n",
    "            print(\"\\nMetrics Summary:\")\n",
    "            print(tabulate(display_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "            # Separate classifiers and confusion matrices\n",
    "            classifier_names = group['Classifier'].tolist()\n",
    "            confusion_matrices = group['Test_Confusion_Matrix'].tolist()\n",
    "\n",
    "            # Build display format for classifiers and confusion matrices\n",
    "            display_confusion_matrix = pd.DataFrame([confusion_matrices], columns=classifier_names, index=[\"Confusion Matrix\"])\n",
    "            # display_classifiers = pd.DataFrame([classifier_names], columns=classifier_names, index=[\"Classifier\"])\n",
    "            # combined_df = pd.concat([display_classifiers, display_confusion_matrix])\n",
    "            print(tabulate(display_confusion_matrix, headers='keys', tablefmt='grid', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndJb7CnsyaGk"
   },
   "outputs": [],
   "source": [
    "# [Cell 4] - Data Processing Functions [data_processor.py]\n",
    "def load_and_split_data(data_dir, split_index=0):\n",
    "    images_by_class = {}\n",
    "    labels_by_class = {}\n",
    "\n",
    "    class_folders = sorted(os.listdir(data_dir))\n",
    "    print(f'> [load_and_split_data] Data preparing from the class: ')\n",
    "    for class_index, class_folder in enumerate(class_folders):\n",
    "        print(f'[{class_index} {class_folder}]', end=' ')\n",
    "        class_path = os.path.join(data_dir, class_folder)\n",
    "        if os.path.isdir(class_path):\n",
    "            img_files = sorted(os.listdir(class_path))\n",
    "\n",
    "            images_by_class[class_index] = []\n",
    "            labels_by_class[class_index] = []\n",
    "\n",
    "            for img_file in img_files:\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                image = load_img(\n",
    "                    img_path,\n",
    "                    target_size=hyperparameters['img_size'],\n",
    "                    color_mode=hyperparameters['color_mode']\n",
    "                )\n",
    "                image = img_to_array(image) / 255.0\n",
    "                images_by_class[class_index].append(image)\n",
    "                labels_by_class[class_index].append(class_index)\n",
    "    print()\n",
    "\n",
    "    train_images, train_labels = [], []\n",
    "    test_images, test_labels = [], []\n",
    "\n",
    "    print(\"\\n=== [DATA DISTRIBUTION] ===\")\n",
    "    print(f\"{'Class':^10} {'Total':^10} {'Train':^10} {'Test':^10} {'Start':^10} {'End':^10}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    test_size = hyperparameters['test_split_size']\n",
    "    for class_idx in sorted(images_by_class.keys()):\n",
    "        X = np.array(images_by_class[class_idx])\n",
    "        y = np.array(labels_by_class[class_idx])\n",
    "\n",
    "        total_samples = len(X)\n",
    "        chunk_size = int(total_samples * test_size)\n",
    "        start_idx = split_index * chunk_size\n",
    "        end_idx = start_idx + chunk_size\n",
    "\n",
    "        test_mask = np.zeros(total_samples, dtype=bool)\n",
    "        test_mask[start_idx:end_idx] = True\n",
    "\n",
    "        test_images.extend(X[test_mask])\n",
    "        test_labels.extend(y[test_mask])\n",
    "        train_images.extend(X[~test_mask])\n",
    "        train_labels.extend(y[~test_mask])\n",
    "\n",
    "        print(f\"{class_idx:^10} {total_samples:^10} {sum(~test_mask):^10} {sum(test_mask):^10} {start_idx:^10} {end_idx-1:^10}\")\n",
    "\n",
    "    return np.array(train_images), np.array(train_labels), np.array(test_images), np.array(test_labels)\n",
    "\n",
    "def save_processed_data(data_dir, split_index):\n",
    "    base_name = f\"{data_dir.split('/')[-2]}_{data_dir.split('/')[-1]}_split_{split_index}\"\n",
    "    save_path = os.path.join(DATA_DIR, base_name)\n",
    "\n",
    "    if os.path.exists(f\"{save_path}_train.pkl\"):\n",
    "        print(f\"> Data already saved in {save_path} (_train.pkl or _test.pkl)\")\n",
    "        return\n",
    "\n",
    "    x_train, y_train, x_test, y_test = load_and_split_data(data_dir, split_index)\n",
    "\n",
    "    with open(f\"{save_path}_train.pkl\", 'wb') as f:\n",
    "        pickle.dump((x_train, y_train), f)\n",
    "    with open(f\"{save_path}_test.pkl\", 'wb') as f:\n",
    "        pickle.dump((x_test, y_test), f)\n",
    "\n",
    "    print(f\"> Saved processed data to {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "def load_processed_data(base_path):\n",
    "    with open(f\"{base_path}_train.pkl\", 'rb') as f:\n",
    "        x_train, y_train = pickle.load(f)\n",
    "    with open(f\"{base_path}_test.pkl\", 'rb') as f:\n",
    "        x_test, y_test = pickle.load(f)\n",
    "\n",
    "    test_size = hyperparameters['test_split_size']\n",
    "    print(\"=== Summary of Loaded Data: ===\")\n",
    "    print(f\"Total images: {len(x_train) + len(x_test)}\")\n",
    "    print(f\"Training set: {len(x_train)} images ({(1-test_size)*100:.0f}%)\")\n",
    "    print(f\"Test set: {len(x_test)} images ({test_size*100:.0f}%)\")\n",
    "    print(f\"Image shape: {x_train[0].shape}\")\n",
    "    print()\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_history(history):\n",
    "    \"\"\"Print training history with timing information\"\"\"\n",
    "\n",
    "    # Create list of lists with epoch number and timing\n",
    "    data: List[List[Union[int, float]]] = []  # Explicitly type the data list\n",
    "    total_time: float = 0.0\n",
    "\n",
    "    for epoch in range(len(history.history['loss'])):\n",
    "        # Initialize row with explicit types\n",
    "        current_row: List[Union[int, float]] = []\n",
    "\n",
    "        # Add epoch number (integer)\n",
    "        current_row.append(int(epoch + 1))\n",
    "\n",
    "        # Add metrics (floats)\n",
    "        for metric in history.history.keys():\n",
    "            if metric not in ['batch', 'size', 'time']:\n",
    "                current_row.append(float(history.history[metric][epoch]))\n",
    "\n",
    "        # Add timing information if available\n",
    "        if hasattr(history, 'epoch_times'):\n",
    "            epoch_time = float(history.epoch_times[epoch])\n",
    "            total_time += epoch_time\n",
    "            current_row.append(float(epoch_time))\n",
    "            current_row.append(float(total_time))\n",
    "\n",
    "        data.append(current_row)\n",
    "\n",
    "    # Create headers\n",
    "    headers: List[str] = ['Epoch']\n",
    "    for key in history.history.keys():\n",
    "        if key not in ['batch', 'size', 'time']:\n",
    "            headers.append(key.replace('_', ' ').title())\n",
    "    if hasattr(history, 'epoch_times'):\n",
    "        headers.extend(['Time (s)', 'Total Time (s)'])\n",
    "\n",
    "    # Print summary header\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{' TRAINING HISTORY ':=^80}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Print table\n",
    "    print(tabulate(data,\n",
    "                  headers=headers,\n",
    "                  floatfmt=('.0f', '.4f', '.4f', '.4f', '.4f', '.3f', '.3f'),\n",
    "                  tablefmt='grid'))\n",
    "\n",
    "    # Print timing summary if available\n",
    "    if hasattr(history, 'epoch_times'):\n",
    "        avg_time = total_time / len(history.epoch_times)\n",
    "        total_time_td = timedelta(seconds=total_time)\n",
    "        avg_time_td = timedelta(seconds=avg_time)\n",
    "        print(\"\\nTiming Summary:\")\n",
    "        print(f\"  • Total Training Time: {total_time_td} (hh:mm:ss)\")\n",
    "        print(f\"  • Average Epoch Time: {avg_time_td} (hh:mm:ss)\")\n",
    "        print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnbguAfByaGl"
   },
   "outputs": [],
   "source": [
    "# Helper function to build U-Net (custom   implementation)\n",
    "def build_unet_model(input_shape=(128, 128, 3)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoding (down-sampling) path\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bridge (bottom of the U-Net)\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "\n",
    "    # Decoding (up-sampling) path\n",
    "    u1 = layers.UpSampling2D((2, 2))(c3)\n",
    "    u1 = layers.Concatenate()([u1, c2])\n",
    "\n",
    "    u2 = layers.UpSampling2D((2, 2))(u1)\n",
    "    u2 = layers.Concatenate()([u2, c1])\n",
    "\n",
    "    outputs = layers.Conv2D(3, (1, 1), activation='softmax')(u2)\n",
    "\n",
    "    return models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQAPAo9ODQ9b"
   },
   "outputs": [],
   "source": [
    "# def print_model_summary(model):\n",
    "#     \"\"\"\n",
    "#     Print model summary in different formats\n",
    "\n",
    "#     Args:\n",
    "#         model: Keras model\n",
    "#     \"\"\"\n",
    "#     # Get model name and type\n",
    "#     model_name = model.name\n",
    "#     model_type = model.__class__.__name__\n",
    "\n",
    "#     print(\"\\n=== Model Architecture ===\")\n",
    "#     print(f\"Model: \\\"{model_name}\\\" ({model_type})\")\n",
    "#     print(\"-\" * 80)\n",
    "\n",
    "#     # Create table data\n",
    "#     table_data = []\n",
    "#     total_params = 0\n",
    "#     trainable_params = 0\n",
    "#     non_trainable_params = 0\n",
    "\n",
    "#     for layer in model.layers:\n",
    "#         layer_type = layer.__class__.__name__\n",
    "#         output_shape = layer.output_shape\n",
    "#         params = layer.count_params()\n",
    "#         trainable = layer.trainable\n",
    "#         trainable_p = sum([w.numpy().size for w in layer.trainable_weights]) if layer.trainable_weights else 0\n",
    "#         non_trainable_p = sum([w.numpy().size for w in layer.non_trainable_weights]) if layer.non_trainable_weights else 0\n",
    "\n",
    "#         table_data.append([\n",
    "#             layer.name,\n",
    "#             layer_type,\n",
    "#             str(output_shape),\n",
    "#             f\"{params:,}\",\n",
    "#             f\"{trainable_p:,}\",\n",
    "#             f\"{non_trainable_p:,}\",\n",
    "#             \"✓\" if trainable else \"✗\"\n",
    "#         ])\n",
    "\n",
    "#         total_params += params\n",
    "#         trainable_params += trainable_p\n",
    "#         non_trainable_params += non_trainable_p\n",
    "\n",
    "#     print(tabulate(table_data,\n",
    "#                     headers=['Layer', 'Type', 'Output Shape', 'Params', 'Trainable P', 'Non-trainable P', 'Trainable'],\n",
    "#                     tablefmt='grid'))\n",
    "\n",
    "#     # Print parameter summary\n",
    "#     print(\"\\nModel Summary:\")\n",
    "#     print(f\"Total Parameters: {total_params:,}\")\n",
    "#     print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "#     print(f\"Non-trainable Parameters: {non_trainable_params:,}\")\n",
    "#     print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "def print_model_summary(model, method='compact'):\n",
    "    \"\"\"Print model summary in different formats\n",
    "    Args:\n",
    "        model: Keras model\n",
    "        method: str, one of ['compact', 'detailed']\n",
    "    \"\"\"\n",
    "    if method == 'compact':\n",
    "        # Get model name and type\n",
    "        model_name = model.name\n",
    "        model_type = model.__class__.__name__\n",
    "\n",
    "        print(\"\\n=== Model Architecture ===\")\n",
    "        print(f\"Model: \\\"{model_name}\\\" ({model_type})\")\n",
    "\n",
    "        # Create table data\n",
    "        table_data = []\n",
    "        total_params = 0\n",
    "        trainable_params = 0\n",
    "        non_trainable_params = 0\n",
    "\n",
    "        for layer in model.layers:\n",
    "            # Get layer type\n",
    "            layer_type = layer.__class__.__name__\n",
    "\n",
    "            # Get output shape safely\n",
    "            if hasattr(layer, 'output_shape'):\n",
    "                output_shape = layer.output_shape\n",
    "            elif hasattr(layer, 'output'):\n",
    "                output_shape = layer.output.shape\n",
    "            else:\n",
    "                output_shape = 'unknown'\n",
    "\n",
    "            # Get parameters\n",
    "            try:\n",
    "                params = layer.count_params()\n",
    "                trainable = layer.trainable\n",
    "                trainable_p = sum([w.numpy().size for w in layer.trainable_weights]) if layer.trainable_weights else 0\n",
    "                non_trainable_p = sum([w.numpy().size for w in layer.non_trainable_weights]) if layer.non_trainable_weights else 0\n",
    "            except:\n",
    "                params = 0\n",
    "                trainable = False\n",
    "                trainable_p = 0\n",
    "                non_trainable_p = 0\n",
    "\n",
    "            table_data.append([\n",
    "                layer.name,\n",
    "                layer_type,\n",
    "                str(output_shape),\n",
    "                f\"{params:,}\",\n",
    "                f\"{trainable_p:,}\",\n",
    "                f\"{non_trainable_p:,}\",\n",
    "                \"✓\" if trainable else \"✗\"\n",
    "            ])\n",
    "\n",
    "            total_params += params\n",
    "            trainable_params += trainable_p\n",
    "            non_trainable_params += non_trainable_p\n",
    "\n",
    "        # Print the table\n",
    "        headers = ['Layer', 'Type', 'Output Shape', 'Params', 'Trainable P', 'Non-trainable P', 'Trainable']\n",
    "        print(tabulate(table_data, headers=headers, tablefmt='grid'))\n",
    "\n",
    "        # Print parameter summary\n",
    "        print(\"\\nModel Summary:\")\n",
    "        print(f\"Total Parameters: {total_params:,}\")\n",
    "        print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "        print(f\"Non-trainable Parameters: {non_trainable_params:,}\")\n",
    "        print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "    elif method == 'detailed':\n",
    "        # Use Keras's built-in summary method\n",
    "        stringlist = []\n",
    "        model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"{' DETAILED MODEL SUMMARY ':=^80}\")\n",
    "        print(\"=\"*80)\n",
    "        print('\\n'.join(stringlist))\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# sys.stdout = sys.__stdout__\n",
    "# # Example usage:\n",
    "# # Run the below cell first \n",
    "# model = build_custom_cnn_model(input_shape=(128, 128, 1), num_classes=4, architecture_name='cnn', fine_tune=True)\n",
    "# # Print both compact and detailed summaries\n",
    "# print_model_summary(model, 'compact')\n",
    "# print_model_summary(model, 'detailed')\n",
    "# print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7te5EHbOyaGl"
   },
   "outputs": [],
   "source": [
    "# [Cell 6] - Model Building Functions\n",
    "def build_custom_cnn_model(input_shape=(128, 128, 1), num_classes=4, architecture_name='cnn', fine_tune=True):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    if architecture_name == 'cnn':\n",
    "        # Uses the values from the hyperparameters dictionary\n",
    "        x = layers.Conv2D(hyperparameters['cnn_filters'][0], hyperparameters['cnn_kernel_size'],\n",
    "                          activation=hyperparameters['cnn_activation_function'], padding=hyperparameters['cnn_padding'])(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D(hyperparameters['cnn_pool_size'])(x)\n",
    "\n",
    "        x = layers.Conv2D(hyperparameters['cnn_filters'][1], hyperparameters['cnn_kernel_size'],\n",
    "                          activation=hyperparameters['cnn_activation_function'], padding=hyperparameters['cnn_padding'])(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D(hyperparameters['cnn_pool_size'])(x)\n",
    "\n",
    "        x = layers.Conv2D(hyperparameters['cnn_filters'][2], hyperparameters['cnn_kernel_size'],\n",
    "                          activation=hyperparameters['cnn_activation_function'], padding=hyperparameters['cnn_padding'])(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D(hyperparameters['cnn_pool_size'])(x)\n",
    "\n",
    "        x = layers.Conv2D(hyperparameters['cnn_filters'][3], hyperparameters['cnn_kernel_size'],\n",
    "                          activation=hyperparameters['cnn_activation_function'], padding=hyperparameters['cnn_padding'])(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "        x = layers.Dense(hyperparameters['cnn_dense_units'], activation=hyperparameters['cnn_activation_function'])(x)\n",
    "        x = layers.Dropout(hyperparameters['cnn_dropout_rate'])(x)\n",
    "\n",
    "        # # AveragePooling2D instead of MaxPooling2D for XLA error for deterministic operations\n",
    "        # x = layers.Conv2D(hyperparameters['cnn_filters'][0], hyperparameters['cnn_kernel_size'],\n",
    "        #                   activation=hyperparameters['cnn_activation_function'], padding=hyperparameters['cnn_padding'])(inputs)\n",
    "        # x = layers.BatchNormalization()(x)\n",
    "        # x = layers.AveragePooling2D(hyperparameters['cnn_pool_size'])(x)  # Changed to AveragePooling2D\n",
    "\n",
    "        # x = layers.Conv2D(hyperparameters['cnn_filters'][1], hyperparameters['cnn_kernel_size'],\n",
    "        #                   activation=hyperparameters['cnn_activation_function'], padding=hyperparameters['cnn_padding'])(x)\n",
    "        # x = layers.BatchNormalization()(x)\n",
    "        # x = layers.AveragePooling2D(hyperparameters['cnn_pool_size'])(x)  # Changed to AveragePooling2D\n",
    "\n",
    "        # x = layers.Conv2D(hyperparameters['cnn_filters'][2], hyperparameters['cnn_kernel_size'],\n",
    "        #                   activation=hyperparameters['cnn_activation_function'], padding=hyperparameters['cnn_padding'])(x)\n",
    "        # x = layers.BatchNormalization()(x)\n",
    "        # x = layers.AveragePooling2D(hyperparameters['cnn_pool_size'])(x)  # Changed to AveragePooling2D\n",
    "\n",
    "        # x = layers.Conv2D(hyperparameters['cnn_filters'][3], hyperparameters['cnn_kernel_size'],\n",
    "        #                   activation=hyperparameters['cnn_activation_function'], padding=hyperparameters['cnn_padding'])(x)\n",
    "        # x = layers.BatchNormalization()(x)\n",
    "        # x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "        # x = layers.Dense(hyperparameters['cnn_dense_units'], activation=hyperparameters['cnn_activation_function'])(x)\n",
    "        # x = layers.Dropout(hyperparameters['cnn_dropout_rate'])(x)\n",
    "    else:\n",
    "        # Upsample input to the required size if needed (InceptionV3 requires minimum 75x75)\n",
    "        if architecture_name == 'inceptionv3' and (input_shape[0] < 75 or input_shape[1] < 75):\n",
    "            required_size = (75, 75)  # InceptionV3 minimum size\n",
    "        else:\n",
    "            required_size = (input_shape[0], input_shape[1])  # Default size for other models\n",
    "\n",
    "        x = layers.Resizing(required_size[0], required_size[1])(inputs)  # Resize input to required size\n",
    "\n",
    "        # Convert grayscale (1-channel) to 3-channel RGB for pretrained models\n",
    "        x = layers.Conv2D(3, (1, 1))(x)\n",
    "\n",
    "        # Pretrained model selection logic\n",
    "        # ResNet Variants\n",
    "        if architecture_name == 'resnet50':\n",
    "            from tensorflow.keras.applications import ResNet50\n",
    "            base_model = ResNet50(include_top=False, weights=hyperparameters['pretained_model_weights'])\n",
    "        elif architecture_name == 'resnet101':\n",
    "            from tensorflow.keras.applications import ResNet101\n",
    "            base_model = ResNet101(include_top=False, weights=hyperparameters['pretained_model_weights'])\n",
    "        elif architecture_name == 'resnet152':\n",
    "            from tensorflow.keras.applications import ResNet152\n",
    "            base_model = ResNet152(include_top=False, weights=hyperparameters['pretained_model_weights'])\n",
    "\n",
    "        # EfficientNetB0\n",
    "        elif architecture_name == 'efficientnetb0':\n",
    "            from tensorflow.keras.applications import EfficientNetB0\n",
    "            base_model = EfficientNetB0(include_top=False, weights=hyperparameters['pretained_model_weights'])\n",
    "\n",
    "        # VGGNet Variants\n",
    "        elif architecture_name == 'vgg16':\n",
    "            from tensorflow.keras.applications import VGG16\n",
    "            base_model = VGG16(include_top=False, weights=hyperparameters['pretained_model_weights'])\n",
    "        elif architecture_name == 'vgg19':\n",
    "            from tensorflow.keras.applications import VGG19\n",
    "            base_model = VGG19(include_top=False, weights=hyperparameters['pretained_model_weights'])\n",
    "\n",
    "        # InceptionV3\n",
    "        elif architecture_name == 'inceptionv3':\n",
    "            from tensorflow.keras.applications import InceptionV3\n",
    "            base_model = InceptionV3(include_top=False, weights=hyperparameters['pretained_model_weights'])\n",
    "\n",
    "        # U-Net (not from Keras applications, custom U-Net function)\n",
    "        elif architecture_name == 'unet':\n",
    "            base_model = build_unet_model(input_shape=(required_size[0], required_size[1], 3))  # Custom function to build U-Net model\n",
    "\n",
    "        # Set base model to non-trainable if fine-tuning is disabled\n",
    "        if not fine_tune:\n",
    "            base_model.trainable = False\n",
    "        else:\n",
    "            base_model.trainable = True\n",
    "\n",
    "        # Apply base model to input\n",
    "        x = base_model(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Compile the model using learning rate and loss from the hyperparameters\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparameters['learning_rate']),\n",
    "        loss= hyperparameters['cnn_loss_function'],\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # print(model.summary())\n",
    "    print_model_summary(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjXTKYynyaGm"
   },
   "outputs": [],
   "source": [
    "# [Cell 7] - Pretext Task Functions\n",
    "def augment_image(image, rotation_angle):\n",
    "    if rotation_angle == 90:\n",
    "        image = tf.image.rot90(image)\n",
    "    elif rotation_angle == 180:\n",
    "        image = tf.image.rot90(image, k=2)\n",
    "    elif rotation_angle == 270:\n",
    "        image = tf.image.rot90(image, k=3)\n",
    "\n",
    "    label = rotation_angle // 90\n",
    "    return image, label\n",
    "\n",
    "def preprocess_data(images):\n",
    "    augmented_images = []\n",
    "    labels = []\n",
    "    for image in images:\n",
    "        for rotation_angle in [0, 90, 180, 270]:\n",
    "            aug_image, label = augment_image(image, rotation_angle)\n",
    "            augmented_images.append(aug_image)\n",
    "            labels.append(label)\n",
    "    print(f'> {len(augmented_images)} augmented images generated each of shape {augmented_images[0].shape} with {len(labels)} labels\\n')\n",
    "    return np.array(augmented_images), np.array(labels)\n",
    "\n",
    "def save_model(model, model_path):\n",
    "    try:\n",
    "        try:\n",
    "            model.save(model_path, save_format='h5')\n",
    "        except Exception as h5_error:\n",
    "            print(f\"> H5 saving failed, trying SavedModel format: {h5_error}\")\n",
    "            model_path = model_path.replace('.h5', '')\n",
    "            model.save(model_path, save_format='tf')\n",
    "\n",
    "        print(f\"> Model successfully saved at: {model_path}\\n\")\n",
    "        return model_path\n",
    "    except Exception as e:\n",
    "        print(f\"> Error saving model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_model(architecture_name, data_path, split_index):\n",
    "    model_name = f\"pretext_model_{os.path.basename(data_path)}_{architecture_name}.h5\"\n",
    "    model_path = os.path.join(MODEL_DIR, model_name)\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(model_path):\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "        else:\n",
    "            model_path = model_path.replace('.h5', '')\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "        print(f\"\\n> Model successfully loaded from {model_path}\\n\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"\\n> Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# To use this, modify your model.fit() call to include timing callback:\n",
    "def run_pretext_pipeline(x_augmented, y_augmented, architecture_name, model_path):\n",
    "    model = build_custom_cnn_model(\n",
    "        input_shape=hyperparameters['input_shape'],\n",
    "        num_classes=hyperparameters['num_classes'],\n",
    "        architecture_name=architecture_name,\n",
    "        fine_tune=hyperparameters['fine_tune']\n",
    "    )\n",
    "\n",
    "    class TimingCallback(tf.keras.callbacks.Callback):\n",
    "        def __init__(self):\n",
    "            self.epoch_times = []\n",
    "            self.epoch_start_time = None\n",
    "\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.epoch_start_time = datetime.now()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            epoch_time = (datetime.now() - self.epoch_start_time).total_seconds()\n",
    "            self.epoch_times.append(epoch_time)\n",
    "\n",
    "    timing_callback = TimingCallback()\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=hyperparameters['early_stopping_patience'],\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=hyperparameters['lr_reduction_factor'],\n",
    "            patience=hyperparameters['lr_reduction_patience']\n",
    "        ),\n",
    "        timing_callback  # Add timing callback\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        x_augmented, y_augmented,\n",
    "        validation_split=hyperparameters['validation_split'],\n",
    "        batch_size=hyperparameters['batch_size'],\n",
    "        epochs=hyperparameters['epochs'],\n",
    "        callbacks=callbacks,\n",
    "        shuffle=True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Add timing information to history object\n",
    "    history.epoch_times = timing_callback.epoch_times\n",
    "\n",
    "    # Save the trained model\n",
    "    save_model(model, model_path)\n",
    "\n",
    "    return history\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7U-tX4GUyaGm"
   },
   "outputs": [],
   "source": [
    "# [Cell 8] - Downstream Task Functions\n",
    "def extract_features(pretext_model, x_data, layer_index=-2):\n",
    "    intermediate_model = models.Model(inputs=pretext_model.input, outputs=pretext_model.layers[layer_index].output)\n",
    "    features = intermediate_model.predict(x_data, verbose=2)\n",
    "    print(f'> extracted features of shape {features.shape}')\n",
    "    return features\n",
    "\n",
    "def evaluate_downstream_task(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average='macro'),\n",
    "        'recall': recall_score(y_test, y_pred, average='macro'),\n",
    "        'f1': f1_score(y_test, y_pred, average='macro'),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "def train_downstream_task(train_features, train_labels, test_features, test_labels, classifier='random_forest', n_splits=None):\n",
    "    # Assign number of splits from hyperparameters if not specified\n",
    "    if n_splits is None:\n",
    "        n_splits = hyperparameters['cv_splits']\n",
    "\n",
    "    # Initialize classifier based on hyperparameters\n",
    "    if classifier == 'random_forest':\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=hyperparameters['rf_n_estimators'],\n",
    "            random_state=RANDOM_SEED,\n",
    "            verbose=0\n",
    "        )\n",
    "    elif classifier == 'svm':\n",
    "        clf = SVC(\n",
    "            kernel=hyperparameters['svm_kernel'],\n",
    "            random_state=RANDOM_SEED,\n",
    "            verbose=False\n",
    "        )\n",
    "    elif classifier == 'gradient_boosting':\n",
    "        clf = GradientBoostingClassifier(\n",
    "            n_estimators=hyperparameters['gb_n_estimators'],\n",
    "            random_state=RANDOM_SEED,\n",
    "            verbose=0\n",
    "        )\n",
    "    elif classifier == 'xgboost':\n",
    "        clf = XGBClassifier(\n",
    "            n_estimators=hyperparameters['xgb_n_estimators'],\n",
    "            random_state=RANDOM_SEED,\n",
    "            use_label_encoder=False,  # Update for recent versions of XGBoost\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    cv_scores = {\n",
    "        'accuracies': [], 'precisions': [], 'recalls': [], 'f1_scores': []\n",
    "    }\n",
    "\n",
    "    for train_idx, val_idx in skf.split(train_features, train_labels):\n",
    "        X_train_fold, X_val_fold = train_features[train_idx], train_features[val_idx]\n",
    "        y_train_fold, y_val_fold = train_labels[train_idx], train_labels[val_idx]\n",
    "\n",
    "        clf.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = clf.predict(X_val_fold)\n",
    "\n",
    "        # Append cross-validation scores\n",
    "        cv_scores['accuracies'].append(accuracy_score(y_val_fold, y_pred))\n",
    "        cv_scores['precisions'].append(precision_score(y_val_fold, y_pred, average='macro'))\n",
    "        cv_scores['recalls'].append(recall_score(y_val_fold, y_pred, average='macro'))\n",
    "        cv_scores['f1_scores'].append(f1_score(y_val_fold, y_pred, average='macro'))\n",
    "\n",
    "    # Train final classifier on full training data and evaluate on test data\n",
    "    clf.fit(train_features, train_labels)\n",
    "    test_metrics = evaluate_downstream_task(clf, test_features, test_labels)\n",
    "\n",
    "    return clf, cv_scores, test_metrics\n",
    "\n",
    "def run_downstream_pipeline(train_features, y_train, test_features, y_test, downstream_classifier):\n",
    "    clf, cv_scores, test_metrics = train_downstream_task(\n",
    "        train_features, y_train,\n",
    "        test_features, y_test,\n",
    "        classifier=downstream_classifier,\n",
    "        n_splits=hyperparameters['cv_splits']\n",
    "    )\n",
    "\n",
    "    return clf, cv_scores, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSl9a4lJyaGn"
   },
   "outputs": [],
   "source": [
    "# [Cell 10] - Main Execution\n",
    "def main():\n",
    "    # Parameters\n",
    "    mode = experiment_parameters['mode']\n",
    "    split_index = experiment_parameters['split_index']\n",
    "    architecture = experiment_parameters['architecture']\n",
    "    classifier = experiment_parameters['classifier']\n",
    "\n",
    "    # Data directory\n",
    "    data_dir = experiment_parameters['data_dir']\n",
    "\n",
    "    try:\n",
    "        # Step 1: Process and save data\n",
    "        if mode in ['process', 'all']:\n",
    "            print(\"\\n=== Processing Data ===\")\n",
    "            data_path = save_processed_data(data_dir, split_index)\n",
    "\n",
    "        # Step 2: Train pretext model\n",
    "        if mode in ['pretext', 'all']:\n",
    "            print(\"\\n=== Training Pretext Model ===\")\n",
    "            data_path = os.path.join(DATA_DIR, f\"{data_dir.split('/')[-2]}_{data_dir.split('/')[-1]}_split_{split_index}\")\n",
    "            x_train, _, _, _ = load_processed_data(data_path)\n",
    "            x_augmented, y_augmented = preprocess_data(x_train)\n",
    "\n",
    "            architectures = architecture_list if architecture == 'all' else [architecture]\n",
    "            for arch in architectures:\n",
    "                model_name = f\"pretext_model_{os.path.basename(data_path)}_{arch}.h5\"\n",
    "                model_path = os.path.join(MODEL_DIR, model_name)\n",
    "\n",
    "                if os.path.exists(model_path):\n",
    "                    print(f\"> Alredy exists in {model_path}. No need for training.\\n\")\n",
    "                    continue\n",
    "\n",
    "                print(f\">> Begin training architecture: {arch}\")\n",
    "                history = run_pretext_pipeline(x_augmented, y_augmented, arch, model_path)\n",
    "                print_training_history(history)\n",
    "                # plot_training_history(history)\n",
    "                print(f\">> End training architecture: {arch}\\n\")\n",
    "\n",
    "            del x_train\n",
    "            del x_augmented\n",
    "            del y_augmented\n",
    "            gc.collect()\n",
    "\n",
    "        # Step 3: Run downstream task\n",
    "        if mode in ['downstream', 'all']:\n",
    "            print(\"\\n=== Running Downstream Task ===\")\n",
    "            data_path = os.path.join(DATA_DIR, f\"{data_dir.split('/')[-2]}_{data_dir.split('/')[-1]}_split_{split_index}\")\n",
    "            x_train, y_train, x_test, y_test = load_processed_data(data_path)\n",
    "\n",
    "            results_tracker = ResultsTracker()\n",
    "            architectures = architecture_list if architecture == 'all' else [architecture]\n",
    "            classifiers = classifier_list if classifier == 'all' else [classifier]\n",
    "\n",
    "            for arch in architectures:\n",
    "                pretext_model = load_model(arch, data_path, split_index)\n",
    "                train_features = extract_features(pretext_model, x_train, hyperparameters['feature_extraction_layer'])\n",
    "                test_features = extract_features(pretext_model, x_test, hyperparameters['feature_extraction_layer'])\n",
    "                print()\n",
    "\n",
    "                for clf in classifiers:\n",
    "                    print(f\">> [STARTED] classifier: {clf} with architecture: {arch}\", end=' ')\n",
    "                    classifier_model, cv_scores, test_metrics = run_downstream_pipeline(\n",
    "                        train_features, y_train, test_features, y_test, clf\n",
    "                    )\n",
    "\n",
    "                    results_tracker.add_result(\n",
    "                        data_dir=data_dir,\n",
    "                        architecture=arch,\n",
    "                        classifier=clf,\n",
    "                        cv_metrics=cv_scores,\n",
    "                        test_metrics=test_metrics\n",
    "                    )\n",
    "                    print(f\"[FINISHED]\")\n",
    "\n",
    "                # results_tracker.display_results()\n",
    "                del pretext_model\n",
    "                del train_features\n",
    "                del test_features\n",
    "                gc.collect()\n",
    "\n",
    "            del x_train\n",
    "            del x_test\n",
    "            del y_train\n",
    "            del y_test\n",
    "            gc.collect()\n",
    "\n",
    "            results_tracker.display_results()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U04153MjyaGn"
   },
   "outputs": [],
   "source": [
    "# # [Cell 11] - Run the pipeline\n",
    "# if __name__ == \"__main__\":\n",
    "#     timestamp = datetime.now().strftime('%Y-%m-%d-%Hh-%Mm-%Ss')\n",
    "#     filename = os.path.join(RESULTS_DIR, f\"{timestamp}-dir_{experiment_parameters['data_dir'].split('/')[-2]}_{experiment_parameters['data_dir'].split('/')[-1]}-mode_{experiment_parameters['mode']}-split_{experiment_parameters['split_index']}-arch_{experiment_parameters['architecture']}-clf_{experiment_parameters['classifier']}.txt\")\n",
    "#     print(f\"> Output is being saved on {filename}\")\n",
    "\n",
    "#     try:\n",
    "#         with open(filename, 'w') as output_file:\n",
    "#             sys.stdout = output_file\n",
    "#             print(filename)\n",
    "#             display_configurations()\n",
    "#             main()\n",
    "#             sys.stdout = sys.__stdout__\n",
    "#     except Exception as e:\n",
    "#         sys.stdout = sys.__stdout__\n",
    "#         print(\"ERROR OCCURED\", e)\n",
    "\n",
    "#     print(\"                                        === PROCESS FINISHED ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MfJ9pxB2LuC"
   },
   "outputs": [],
   "source": [
    "# Create a MultiWriter class that writes to both console and file\n",
    "class MultiWriter:\n",
    "    def __init__(self, filename):\n",
    "        self.log = open(filename, 'w')\n",
    "        self.out = Output()  # Output widget to display output in the notebook\n",
    "        display(self.out)    # Display the widget in the notebook cell\n",
    "\n",
    "    def write(self, message):\n",
    "        # Write to the file\n",
    "        self.log.write(message)\n",
    "        self.log.flush()\n",
    "\n",
    "        # Write to the notebook cell output without recursion\n",
    "        with self.out:\n",
    "            self.out.append_stdout(message)  # Avoids calling print()\n",
    "\n",
    "    def flush(self):\n",
    "        self.log.flush()\n",
    "\n",
    "def setup_directories():\n",
    "    try:\n",
    "        # Create base experiment directory\n",
    "        if not os.path.exists(EXPERIMENT_BASE_DIR):\n",
    "            os.makedirs(EXPERIMENT_BASE_DIR)\n",
    "\n",
    "        # Create subdirectories\n",
    "        subdirs = {\n",
    "            'MODEL_DIR': MODEL_DIR,\n",
    "            'DATA_DIR': DATA_DIR,\n",
    "            'RESULTS_DIR': RESULTS_DIR\n",
    "        }\n",
    "\n",
    "        for name, path in subdirs.items():\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating directories: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "dfdc386a95824b388bda48c0d8a4d89b",
      "5dfc89e9e0d344bba8a56c7c875f3436"
     ]
    },
    "id": "zE2gDrRZ0SPl",
    "outputId": "96a8db9f-b5ae-4173-82c0-bc01adcd9c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Output will be saved to: /content/results/2024-11-03-15h-06m-49s-dir_qpm_real-mode_all-split_0-arch_all-clf_all.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdc386a95824b388bda48c0d8a4d89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=h5\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=h5\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Modify the main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # First ensure directories exist\n",
    "    if not setup_directories():\n",
    "        print(\"Failed to create necessary directories. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # set_all_seeds(RANDOM_SEED, gpu_deterministic=False)\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d-%Hh-%Mm-%Ss')\n",
    "    filename = os.path.join(RESULTS_DIR, f\"{timestamp}-dir_{experiment_parameters['data_dir'].split('/')[-2]}_{experiment_parameters['data_dir'].split('/')[-1]}-mode_{experiment_parameters['mode']}-split_{experiment_parameters['split_index']}-arch_{experiment_parameters['architecture']}-clf_{experiment_parameters['classifier']}.txt\")\n",
    "\n",
    "    print(f\"> Output will be saved to: {filename}\")\n",
    "\n",
    "    try:\n",
    "        sys.stdout = MultiWriter(filename)\n",
    "\n",
    "        check_environment()\n",
    "\n",
    "        print(\"\\n=== Starting Experiment ===\")\n",
    "        display_configurations()\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        main()\n",
    "        end_time = datetime.now()\n",
    "\n",
    "        duration = end_time - start_time\n",
    "        print(f\"\\nStart Time: {start_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}\")  # Show milliseconds\n",
    "        print(f\"End Time: {end_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}\")      # Show milliseconds\n",
    "        # Formatting duration as hh:mm:ss.sss\n",
    "        total_seconds = duration.total_seconds()\n",
    "        hours, remainder = divmod(total_seconds, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        milliseconds = duration.microseconds // 1000\n",
    "        print(f\"Duration: {int(hours):02}:{int(minutes):02}:{int(seconds):02}.{milliseconds:03}\")\n",
    "        print(\"\\n=== Experiment Finished Successfully ===\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # Make sure to restore stdout and close the file\n",
    "        if isinstance(sys.stdout, MultiWriter):\n",
    "            sys.stdout.log.close()\n",
    "        sys.stdout = sys.__stdout__\n",
    "        print(\"=== PROCESS FINISHED ===\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "5dfc89e9e0d344bba8a56c7c875f3436": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfdc386a95824b388bda48c0d8a4d89b": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_5dfc89e9e0d344bba8a56c7c875f3436",
      "msg_id": "4bb81807-b6cc-43d4-e900-5d478e03b046",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n===================================================================================================="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "================================ LIBRARY VERSIONS AND REQUIREMENTS ================================="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "===================================================================================================="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "+----------------+-------------------+--------------------+----------+\n| Library        | Current Version   | Required Version   | Status   |\n+================+===================+====================+==========+\n| Python         | 3.10.12           | N/A                | ---      |\n+----------------+-------------------+--------------------+----------+\n| TensorFlow     | 2.17.0            | 2.8.0              | ✗        |\n+----------------+-------------------+--------------------+----------+\n| NumPy          | 1.26.4            | 1.19.2             | ✗        |\n+----------------+-------------------+--------------------+----------+\n| Pandas         | 2.2.2             | 2.2.3              | ✗        |\n+----------------+-------------------+--------------------+----------+\n| Scikit-learn   | 1.5.2             | 0.24.2             | ✗        |\n+----------------+-------------------+--------------------+----------+\n| XGBoost        | 2.1.2             | 1.5.0              | ✗        |\n+----------------+-------------------+--------------------+----------+\n| Matplotlib     | 3.8.0             | 3.4.3              | ✗        |\n+----------------+-------------------+--------------------+----------+\n| Pillow (PIL)   | 10.4.0            | 8.3.2              | ✗        |\n+----------------+-------------------+--------------------+----------+\n| Tabulate       | 0.9.0             | 0.9.0              | ✓        |\n+----------------+-------------------+--------------------+----------+\n| IPython        | Not installed     | 7.31.1             | ?        |\n+----------------+-------------------+--------------------+----------+\n| IPykernel      | 5.5.6             | 6.29.5             | ✗        |\n+----------------+-------------------+--------------------+----------+\n| IPywidgets     | 7.7.1             | 8.1.5              | ✗        |\n+----------------+-------------------+--------------------+----------+\n| Jupyter Client | 6.1.12            | 8.6.3              | ✗        |\n+----------------+-------------------+--------------------+----------+\n| Jupyter Core   | 5.7.2             | 5.7.2              | ✓        |\n+----------------+-------------------+--------------------+----------+\n| Jupyter Server | 1.24.0            | 2.14.2             | ✗        |\n+----------------+-------------------+--------------------+----------+\n| JupyterLab     | Not installed     | 4.2.5              | ?        |\n+----------------+-------------------+--------------------+----------+\n| NBClient       | 0.10.0            | 0.10.0             | ✓        |\n+----------------+-------------------+--------------------+----------+\n| NBConvert      | 7.16.4            | 7.16.4             | ✓        |\n+----------------+-------------------+--------------------+----------+\n| NBFormat       | 5.10.4            | 5.10.4             | ✓        |\n+----------------+-------------------+--------------------+----------+\n| Notebook       | 6.5.5             | 7.2.2              | ✗        |\n+----------------+-------------------+--------------------+----------+\n| Traitlets      | 5.7.1             | 5.14.3             | ✗        |\n+----------------+-------------------+--------------------+----------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n===================================================================================================="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "======================================== SYSTEM INFORMATION ========================================"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "===================================================================================================="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "OS: Linux #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\nGPU Information:"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "CUDA Available: True"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Found 1 GPU(s):"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "  • PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\nTensorFlow Environment Variables:"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "  • TF_FORCE_GPU_ALLOW_GROWTH: true"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "  • TF_CPP_MIN_LOG_LEVEL: 1"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "====================================================================================================\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n=== Starting Experiment ==="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n========== Experiment Configurations =========="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n>>> Architectures <<<"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "+--------------------------------------------------------------------------------------+\n| cnn, resnet50, resnet101, resnet152, efficientnetb0, vgg16, vgg19, inceptionv3, unet |\n+--------------------------------------------------------------------------------------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n>>> Classifiers <<<"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "+------------------------------------------------+\n| random_forest, svm, gradient_boosting, xgboost |\n+------------------------------------------------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n>>> THIS EXPERIMENT PIPELINE <<<"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "+--------------+----------------------------------------------------+\n| Parameter    | Value                                              |\n+==============+====================================================+\n| mode         | all                                                |\n+--------------+----------------------------------------------------+\n| split_index  | 0                                                  |\n+--------------+----------------------------------------------------+\n| architecture | all                                                |\n+--------------+----------------------------------------------------+\n| classifier   | all                                                |\n+--------------+----------------------------------------------------+\n| data_dir     | /content/SAMPLE_dataset_public/png_images/qpm/real |\n+--------------+----------------------------------------------------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n>>> Data Loading and Splitting <<<\n+------------------+------------+\n| Parameter        | Value      |\n+==================+============+\n| img_size         | (128, 128) |\n+------------------+------------+\n| color_mode       | grayscale  |\n+------------------+------------+\n| test_split_size  | 0.2        |\n+------------------+------------+\n| test_split_index | 0          |\n+------------------+------------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n>>> Model Architecture <<<\n+-------------+---------------+\n| Parameter   | Value         |\n+=============+===============+\n| input_shape | (128, 128, 1) |\n+-------------+---------------+\n| num_classes | 4             |\n+-------------+---------------+\n| fine_tune   | True          |\n+-------------+---------------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n>>> CNN Architecture <<<\n+-------------------------+---------------------------------+\n| Parameter               | Value                           |\n+=========================+=================================+\n| cnn_filters             | [32, 64, 128, 256]              |\n+-------------------------+---------------------------------+\n| cnn_kernel_size         | (3, 3)                          |\n+-------------------------+---------------------------------+\n| cnn_pool_size           | (2, 2)                          |\n+-------------------------+---------------------------------+\n| cnn_dense_units         | 512                             |\n+-------------------------+---------------------------------+\n| cnn_dropout_rate        | 0.5                             |\n+-------------------------+---------------------------------+\n| cnn_loss_function       | sparse_categorical_crossentropy |\n+-------------------------+---------------------------------+\n| cnn_activation_function | relu                            |\n+-------------------------+---------------------------------+\n| cnn_padding             | same                            |\n+-------------------------+---------------------------------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n>>> Other Pretext Architectures <<<\n+-------------------------+----------+\n| Parameter               | Value    |\n+=========================+==========+\n| pretained_model_weights | imagenet |\n+-------------------------+----------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n>>> Training Configuration <<<\n+-------------------------+---------+\n| Parameter               |   Value |\n+=========================+=========+\n| batch_size              |  32     |\n+-------------------------+---------+\n| epochs                  |  10     |\n+-------------------------+---------+\n| validation_split        |   0.25  |\n+-------------------------+---------+\n| learning_rate           |   0.001 |\n+-------------------------+---------+\n| early_stopping_patience |   3     |\n+-------------------------+---------+\n| lr_reduction_patience   |   3     |\n+-------------------------+---------+\n| lr_reduction_factor     |   0.5   |\n+-------------------------+---------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n>>> Downstream Task Configuration <<<\n+--------------------------+---------+\n| Parameter                | Value   |\n+==========================+=========+\n| feature_extraction_layer | -2      |\n+--------------------------+---------+\n| rf_n_estimators          | 100     |\n+--------------------------+---------+\n| svm_kernel               | linear  |\n+--------------------------+---------+\n| gb_n_estimators          | 100     |\n+--------------------------+---------+\n| xgb_n_estimators         | 100     |\n+--------------------------+---------+\n| cv_splits                | 5       |\n+--------------------------+---------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n============================================\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n=== Processing Data ==="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "> [load_and_split_data] Data preparing from the class: "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "[0 2s1]"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": " "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "[1 bmp2]"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": " "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "[2 btr70]"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": " "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "[3 m1]"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": " "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "[4 m2]"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": " "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "[5 m35]"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": " "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "[6 m548]"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": " "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "[7 m60]"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": " "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "[8 t72]"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": " "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "[9 zsu23]"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": " "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n=== [DATA DISTRIBUTION] ==="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "  Class      Total      Train       Test      Start       End    "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "------------------------------------------------------------"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "    0         174        140         34         0          33    "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "    1         107         86         21         0          20    "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "    2          92         74         18         0          17    "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "    3         129        104         25         0          24    "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "    4         128        103         25         0          24    "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "    5         129        104         25         0          24    "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "    6         128        103         25         0          24    "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "    7         176        141         35         0          34    "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "    8         108         87         21         0          20    "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "    9         174        140         34         0          33    "
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "> Saved processed data to /content/processed_data/qpm_real_split_0"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n=== Training Pretext Model ==="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "=== Summary of Loaded Data: ==="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Total images: 1345"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Training set: 1082 images (80%)"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Test set: 263 images (20%)"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Image shape: (128, 128, 1)"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "> 4328 augmented images generated each of shape (128, 128, 1) with 4328 labels\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": ">> Begin training architecture: cnn"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n=== Model Architecture ==="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Model: \"functional\" (Functional)"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| Layer                    | Type                   | Output Shape         | Params   | Trainable P   |   Non-trainable P | Trainable   |\n+==========================+========================+======================+==========+===============+===================+=============+\n| input_layer              | InputLayer             | (None, 128, 128, 1)  | 0        | 0             |                 0 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| conv2d                   | Conv2D                 | (None, 128, 128, 32) | 320      | 320           |                 0 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| batch_normalization      | BatchNormalization     | (None, 128, 128, 32) | 128      | 64            |                64 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| max_pooling2d            | MaxPooling2D           | (None, 64, 64, 32)   | 0        | 0             |                 0 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| conv2d_1                 | Conv2D                 | (None, 64, 64, 64)   | 18,496   | 18,496        |                 0 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| batch_normalization_1    | BatchNormalization     | (None, 64, 64, 64)   | 256      | 128           |               128 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| max_pooling2d_1          | MaxPooling2D           | (None, 32, 32, 64)   | 0        | 0             |                 0 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| conv2d_2                 | Conv2D                 | (None, 32, 32, 128)  | 73,856   | 73,856        |                 0 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| batch_normalization_2    | BatchNormalization     | (None, 32, 32, 128)  | 512      | 256           |               256 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| max_pooling2d_2          | MaxPooling2D           | (None, 16, 16, 128)  | 0        | 0             |                 0 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| conv2d_3                 | Conv2D                 | (None, 16, 16, 256)  | 295,168  | 295,168       |                 0 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| batch_normalization_3    | BatchNormalization     | (None, 16, 16, 256)  | 1,024    | 512           |               512 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| global_average_pooling2d | GlobalAveragePooling2D | (None, 256)          | 0        | 0             |                 0 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| dense                    | Dense                  | (None, 512)          | 131,584  | 131,584       |                 0 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| dropout                  | Dropout                | (None, 512)          | 0        | 0             |                 0 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+\n| dense_1                  | Dense                  | (None, 4)            | 2,052    | 2,052         |                 0 | ✓           |\n+--------------------------+------------------------+----------------------+----------+---------------+-------------------+-------------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\nModel Summary:"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Total Parameters: 523,396"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Trainable Parameters: 522,436"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Non-trainable Parameters: 960"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "--------------------------------------------------------------------------------\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "> Model successfully saved at: /content/pretext_models/pretext_model_qpm_real_split_0_cnn.h5\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n================================================================================"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "=============================== TRAINING HISTORY ==============================="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "================================================================================"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|   Epoch |   Accuracy |   Loss |   Val Accuracy |   Val Loss |   Learning Rate |   Time (s) |   Total Time (s) |\n+=========+============+========+================+============+=================+============+==================+\n|       1 |     0.7933 | 0.5679 |         0.2495 |     4.8551 |           0.001 |     27.762 |          27.7625 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|       2 |     0.9960 | 0.0259 |         0.3207 |     1.5072 |           0.001 |     18.436 |          46.1986 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|       3 |     0.9975 | 0.0144 |         0.3669 |     2.5165 |           0.001 |      1.983 |          48.1815 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|       4 |     1.0000 | 0.0024 |         0.4917 |     1.3417 |           0.001 |      2.678 |          50.8591 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|       5 |     1.0000 | 0.0013 |         0.6608 |     1.0781 |           0.001 |      2.635 |          53.4945 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|       6 |     1.0000 | 0.0007 |         0.8725 |     0.2789 |           0.001 |      2.448 |          55.9425 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|       7 |     0.9932 | 0.0182 |         0.2505 |    18.2860 |           0.001 |      2.398 |          58.341  |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|       8 |     0.9991 | 0.0050 |         0.8096 |     0.4873 |           0.001 |      2.545 |          60.8856 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|       9 |     1.0000 | 0.0014 |         0.9898 |     0.0484 |           0.001 |      1.974 |          62.8594 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|      10 |     1.0000 | 0.0007 |         0.9991 |     0.0141 |           0.001 |      2.050 |          64.9093 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\nTiming Summary:"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "  • Total Training Time: 0:01:04.909283 (hh:mm:ss)"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "  • Average Epoch Time: 0:00:06.490928 (hh:mm:ss)"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "================================================================================\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": ">> End training architecture: cnn\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": ">> Begin training architecture: resnet50"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\r\u001b[1m       0/94765736\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0s/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 4202496/94765736\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m10887168/94765736\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m17866752/94765736\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m25419776/94765736\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m33325056/94765736\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m42598400/94765736\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m51126272/94765736\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m60358656/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m68665344/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m76087296/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m82247680/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m89047040/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n=== Model Architecture ==="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Model: \"functional_1\" (Functional)"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "+----------------------------+------------------------+--------------------------+------------+---------------+-------------------+-------------+\n| Layer                      | Type                   | Output Shape             | Params     | Trainable P   | Non-trainable P   | Trainable   |\n+============================+========================+==========================+============+===============+===================+=============+\n| input_layer_1              | InputLayer             | (None, 128, 128, 1)      | 0          | 0             | 0                 | ✓           |\n+----------------------------+------------------------+--------------------------+------------+---------------+-------------------+-------------+\n| resizing                   | Resizing               | (None, 128, 128, 1)      | 0          | 0             | 0                 | ✓           |\n+----------------------------+------------------------+--------------------------+------------+---------------+-------------------+-------------+\n| conv2d_4                   | Conv2D                 | (None, 128, 128, 3)      | 6          | 6             | 0                 | ✓           |\n+----------------------------+------------------------+--------------------------+------------+---------------+-------------------+-------------+\n| resnet50                   | Functional             | (None, None, None, 2048) | 23,587,712 | 23,534,592    | 53,120            | ✓           |\n+----------------------------+------------------------+--------------------------+------------+---------------+-------------------+-------------+\n| global_average_pooling2d_1 | GlobalAveragePooling2D | (None, 2048)             | 0          | 0             | 0                 | ✓           |\n+----------------------------+------------------------+--------------------------+------------+---------------+-------------------+-------------+\n| dense_2                    | Dense                  | (None, 4)                | 8,196      | 8,196         | 0                 | ✓           |\n+----------------------------+------------------------+--------------------------+------------+---------------+-------------------+-------------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\nModel Summary:"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Total Parameters: 23,595,914"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Trainable Parameters: 23,542,794"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Non-trainable Parameters: 53,120"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "--------------------------------------------------------------------------------\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "> Model successfully saved at: /content/pretext_models/pretext_model_qpm_real_split_0_resnet50.h5\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n================================================================================"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "=============================== TRAINING HISTORY ==============================="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "================================================================================"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|   Epoch |   Accuracy |   Loss |   Val Accuracy |   Val Loss |   Learning Rate |   Time (s) |   Total Time (s) |\n+=========+============+========+================+============+=================+============+==================+\n|       1 |     0.9735 | 0.0857 |         0.2505 |   341.0673 |           0.001 |    110.442 |          110.442 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|       2 |     0.9997 | 0.0011 |         0.2495 |     3.4707 |           0.001 |     11.614 |          122.056 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|       3 |     1.0000 | 0.0001 |         0.2495 |     1.6351 |           0.001 |     20.496 |          142.551 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|       4 |     0.9997 | 0.0111 |         0.2495 |     3.9499 |           0.001 |     11.518 |          154.069 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|       5 |     0.9945 | 0.0305 |         0.2505 |    93.6826 |           0.001 |     20.584 |          174.653 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+\n|       6 |     0.9985 | 0.0041 |         0.2495 |     2.0717 |           0.001 |     12.087 |          186.741 |\n+---------+------------+--------+----------------+------------+-----------------+------------+------------------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\nTiming Summary:"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "  • Total Training Time: 0:03:06.740579 (hh:mm:ss)"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "  • Average Epoch Time: 0:00:31.123430 (hh:mm:ss)"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "================================================================================\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": ">> End training architecture: resnet50\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": ">> Begin training architecture: resnet101"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\r\u001b[1m        0/171446536\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0s/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  4202496/171446536\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 11788288/171446536\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 17252352/171446536\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 25165824/171446536\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 26836992/171446536\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 29261824/171446536\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 35979264/171446536\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 41951232/171446536\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 50462720/171446536\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 58728448/171446536\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 68673536/171446536\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 77930496/171446536\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 88080384/171446536\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 95100928/171446536\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m100671488/171446536\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m109060096/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m113696768/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m118906880/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m122568704/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m130260992/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m138248192/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m144539648/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m151093248/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m157417472/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m167550976/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 0us/step"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m171446536/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n=== Model Architecture ==="
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "Model: \"functional_2\" (Functional)"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "+----------------------------+------------------------+--------------------------+------------+---------------+-------------------+-------------+\n| Layer                      | Type                   | Output Shape             | Params     | Trainable P   | Non-trainable P   | Trainable   |\n+============================+========================+==========================+============+===============+===================+=============+\n| input_layer_3              | InputLayer             | (None, 128, 128, 1)      | 0          | 0             | 0                 | ✓           |\n+----------------------------+------------------------+--------------------------+------------+---------------+-------------------+-------------+\n| resizing_1                 | Resizing               | (None, 128, 128, 1)      | 0          | 0             | 0                 | ✓           |\n+----------------------------+------------------------+--------------------------+------------+---------------+-------------------+-------------+\n| conv2d_5                   | Conv2D                 | (None, 128, 128, 3)      | 6          | 6             | 0                 | ✓           |\n+----------------------------+------------------------+--------------------------+------------+---------------+-------------------+-------------+\n| resnet101                  | Functional             | (None, None, None, 2048) | 42,658,176 | 42,552,832    | 105,344           | ✓           |\n+----------------------------+------------------------+--------------------------+------------+---------------+-------------------+-------------+\n| global_average_pooling2d_2 | GlobalAveragePooling2D | (None, 2048)             | 0          | 0             | 0                 | ✓           |\n+----------------------------+------------------------+--------------------------+------------+---------------+-------------------+-------------+\n| dense_3                    | Dense                  | (None, 4)                | 8,196      | 8,196         | 0                 | ✓           |\n+----------------------------+------------------------+--------------------------+------------+---------------+-------------------+-------------+"
       },
       {
        "name": "stdout",
        "output_type": "stream",
        "text": "\n"
       }
      ]
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
