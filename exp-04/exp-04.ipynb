{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 14:29:43.928023: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-01 14:29:44.642991: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-01 14:29:44.664636: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-11-01 14:29:44.664699: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-01 14:29:46.611659: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-01 14:29:46.611909: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-01 14:29:46.611920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# [Cell 1] - Import statements\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from tabulate import tabulate\n",
    "from typing import Dict, List, Any\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 2] - Set global constants and configurations [config.py]\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "EXPERIMENT_BASE_DIR = 'exp-04'\n",
    "MODEL_DIR = os.path.join(EXPERIMENT_BASE_DIR, \"pretext_models\")\n",
    "DATA_DIR = os.path.join(EXPERIMENT_BASE_DIR, \"processed_data\")\n",
    "RESULTS_DIR = os.path.join(EXPERIMENT_BASE_DIR, \"results\")\n",
    "\n",
    "architecture_list = [\n",
    "    'cnn',\n",
    "    'resnet50',\n",
    "    'resnet101',\n",
    "    # 'resnet152',\n",
    "    # 'efficientnetb0',\n",
    "    # 'vgg16',\n",
    "    # 'vgg19',\n",
    "    # 'inceptionv3',\n",
    "    # 'unet'\n",
    "]\n",
    "\n",
    "classifier_list = [\n",
    "    'random_forest',\n",
    "    'svm', \n",
    "    'gradient_boosting',\n",
    "    'xgboost'\n",
    "]\n",
    "\n",
    "experiment_parameters = {\n",
    "    'mode': 'all',  # choices: 'process', 'pretext', 'downstream', 'all'\n",
    "    'split_index': 0,\n",
    "    'architecture': 'all',  # options: specific architecture like 'cnn' or 'all'\n",
    "    'classifier': 'all',  # choices: 'random_forest', 'svm', 'gradient_boosting', 'xgboost'\n",
    "    'data_dir': '\"D:/SAMPLE_dataset_public/png_images/qpm/real\"'\n",
    "}\n",
    "\n",
    "# Configuration dictionary\n",
    "hyperparameters = {\n",
    "    # Data loading and splitting\n",
    "    'img_size': (128, 128),\n",
    "    'color_mode': 'grayscale',\n",
    "    'test_split_size': 0.2,\n",
    "    'test_split_index': 0,\n",
    "\n",
    "    # Model architecture\n",
    "    'input_shape': (128, 128, 1),\n",
    "    'num_classes': 4,\n",
    "    'fine_tune': True,\n",
    "\n",
    "    # CNN architecture\n",
    "    'cnn_filters': [32, 64, 128, 256],\n",
    "    'cnn_kernel_size': (3, 3),\n",
    "    'cnn_pool_size': (2, 2),\n",
    "    'cnn_dense_units': 512,\n",
    "    'cnn_dropout_rate': 0.5,\n",
    "    'cnn_loss_function': 'sparse_categorical_crossentropy',\n",
    "    'cnn_activation_function': 'relu',\n",
    "    'cnn_padding': 'same',\n",
    "\n",
    "    # Other Pretext Architectures\n",
    "    'pretained_model_weights': 'imagenet',\n",
    "\n",
    "    # Training configuration\n",
    "    'batch_size': 32,\n",
    "    'epochs': 10,\n",
    "    'validation_split': 0.25,\n",
    "    'learning_rate': 0.001,\n",
    "    'early_stopping_patience': 3,\n",
    "    'lr_reduction_patience': 3,\n",
    "    'lr_reduction_factor': 0.5,\n",
    "\n",
    "    # Downstream task\n",
    "    'feature_extraction_layer': -2,\n",
    "    'rf_n_estimators': 100,\n",
    "    'svm_kernel': 'linear',\n",
    "    'gb_n_estimators': 100,\n",
    "    'xgb_n_estimators': 100,\n",
    "    'cv_splits': 5,\n",
    "}\n",
    "\n",
    "def display_configurations():\n",
    "    def create_section_table(keys, header):\n",
    "        table = [[key, hyperparameters[key]] for key in keys]\n",
    "        return f\"\\n{header}\\n\" + tabulate(table, headers=['Parameter', 'Value'], tablefmt='grid')\n",
    "\n",
    "    print(\"\\n========== Experiment Configurations ==========\")\n",
    "\n",
    "    # Architectures and Classifiers\n",
    "    print(\"\\n>>> Architectures <<<\")\n",
    "    print(tabulate([[\", \".join(architecture_list)]], tablefmt='grid'))\n",
    "    \n",
    "    print(\"\\n>>> Classifiers <<<\")\n",
    "    print(tabulate([[\", \".join(classifier_list)]], tablefmt='grid'))\n",
    "    \n",
    "    # Experiment Pipeline\n",
    "    print(\"\\n>>> THIS EXPERIMENT PIPELINE <<<\")\n",
    "    table = [[key, value] for key, value in experiment_parameters.items()]\n",
    "    print(tabulate(table, headers=['Parameter', 'Value'], tablefmt='grid'))\n",
    "    \n",
    "    # Data Loading and Splitting\n",
    "    data_keys = ['img_size', 'color_mode', 'test_split_size', 'test_split_index']\n",
    "    print(create_section_table(data_keys, \">>> Data Loading and Splitting <<<\"))\n",
    "    \n",
    "    # Model Architecture\n",
    "    model_keys = ['input_shape', 'num_classes', 'fine_tune']\n",
    "    print(create_section_table(model_keys, \">>> Model Architecture <<<\"))\n",
    "    \n",
    "    # CNN Architecture\n",
    "    cnn_keys = ['cnn_filters', 'cnn_kernel_size', 'cnn_pool_size', 'cnn_dense_units', \n",
    "                'cnn_dropout_rate', 'cnn_loss_function', 'cnn_activation_function', 'cnn_padding']\n",
    "    print(create_section_table(cnn_keys, \">>> CNN Architecture <<<\"))\n",
    "    \n",
    "    # Other Pretext Architectures\n",
    "    pretext_keys = ['pretained_model_weights']\n",
    "    print(create_section_table(pretext_keys, \">>> Other Pretext Architectures <<<\"))\n",
    "    \n",
    "    # Training Configuration\n",
    "    training_keys = ['batch_size', 'epochs', 'validation_split', 'learning_rate',\n",
    "                    'early_stopping_patience', 'lr_reduction_patience', 'lr_reduction_factor']\n",
    "    print(create_section_table(training_keys, \">>> Training Configuration <<<\"))\n",
    "    \n",
    "    # Downstream Task Configuration\n",
    "    downstream_keys = ['feature_extraction_layer', 'rf_n_estimators', 'svm_kernel',\n",
    "                      'gb_n_estimators', 'xgb_n_estimators', 'cv_splits']\n",
    "    print(create_section_table(downstream_keys, \">>> Downstream Task Configuration <<<\"))\n",
    "    \n",
    "    print(\"\\n============================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 3] - Utility functions [seeds.py/ResultsTracker.py]\n",
    "def set_all_seeds(seed=42, gpu_deterministic=True):\n",
    "    \"\"\"Set all seeds to make results reproducible\n",
    "    Example:\n",
    "        # For development/debugging (slower but reproducible)\n",
    "        set_all_seeds(42, gpu_deterministic=True)\n",
    "\n",
    "        # For production/training (faster but not fully reproducible)\n",
    "        set_all_seeds(42, gpu_deterministic=False)\n",
    "    \"\"\"\n",
    "    # Basic seeds\n",
    "    random.seed(seed)                            # Python random module\n",
    "    np.random.seed(seed)                         # Numpy\n",
    "    tf.random.set_seed(seed)                     # TensorFlow\n",
    "    tf.keras.utils.set_random_seed(seed)         # Keras\n",
    "    tf.experimental.numpy.random.seed(seed)      # TensorFlow numpy\n",
    "    \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    if gpu_deterministic:\n",
    "        # GPU-specific deterministic settings\n",
    "        tf.config.experimental.enable_op_determinism()\n",
    "        os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "        os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "        \n",
    "        # Threading controls\n",
    "        tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "        tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "class ResultsTracker:\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "\n",
    "    def add_result(self, data_dir: str, architecture: str, classifier: str, \n",
    "                   cv_metrics: Dict[str, List[float]], test_metrics: Dict[str, Any]):\n",
    "        result = {\n",
    "            'Dataset': f\"{data_dir.split('/')[-2]}_{data_dir.split('/')[-1]}_test_split_ind_{hyperparameters['test_split_index']}\",\n",
    "            'Architecture': architecture,\n",
    "            'Classifier': classifier,\n",
    "            'CV_Accuracy': f\"{np.mean(cv_metrics['accuracies'])*100:.1f}±{np.std(cv_metrics['accuracies'])*100:.1f}\",\n",
    "            'CV_Precision': f\"{np.mean(cv_metrics['precisions'])*100:.1f}±{np.std(cv_metrics['precisions'])*100:.1f}\",\n",
    "            'CV_Recall': f\"{np.mean(cv_metrics['recalls'])*100:.1f}±{np.std(cv_metrics['recalls'])*100:.1f}\",\n",
    "            'CV_F1': f\"{np.mean(cv_metrics['f1_scores'])*100:.1f}±{np.std(cv_metrics['f1_scores'])*100:.1f}\",\n",
    "            'Test_Accuracy': f\"{test_metrics['accuracy']*100:.1f}\",\n",
    "            'Test_Precision': f\"{test_metrics['precision']*100:.1f}\",\n",
    "            'Test_Recall': f\"{test_metrics['recall']*100:.1f}\",\n",
    "            'Test_F1': f\"{test_metrics['f1']*100:.1f}\",\n",
    "            'Test_Confusion_Matrix': f\"{test_metrics['confusion_matrix']}\"\n",
    "        }\n",
    "        self.results.append(result)\n",
    "\n",
    "\n",
    "    def display_results(self):\n",
    "        if not self.results:\n",
    "            print(\"> No results to display\")\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(self.results)\n",
    "        grouped = df.groupby(['Dataset', 'Architecture'])\n",
    "\n",
    "        for (dataset, arch), group in grouped:\n",
    "            print(f\"\\n=== Results: Dataset: <{dataset}> | Pretext Model: <{arch}> ===\")\n",
    "\n",
    "            # Display test and cross-validation metrics\n",
    "            display_cols = [\n",
    "                'Classifier',\n",
    "                'Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1',\n",
    "                'CV_Accuracy', 'CV_Precision', 'CV_Recall', 'CV_F1'\n",
    "            ]\n",
    "            \n",
    "            display_df = group[display_cols].copy()\n",
    "            print(\"\\nMetrics Summary:\")\n",
    "            print(tabulate(display_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "            # Separate classifiers and confusion matrices\n",
    "            classifier_names = group['Classifier'].tolist()\n",
    "            confusion_matrices = group['Test_Confusion_Matrix'].tolist()\n",
    "\n",
    "            # Build display format for classifiers and confusion matrices\n",
    "            display_confusion_matrix = pd.DataFrame([confusion_matrices], columns=classifier_names, index=[\"Confusion Matrix\"])\n",
    "            display_classifiers = pd.DataFrame([classifier_names], columns=classifier_names, index=[\"Classifier\"])\n",
    "            combined_df = pd.concat([display_classifiers, display_confusion_matrix])\n",
    "            print(tabulate(combined_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "def print_training_history(history):\n",
    "    # Create list of lists with epoch number\n",
    "    data = []\n",
    "    for epoch in range(len(history.history['loss'])):\n",
    "        row = [epoch + 1]  # epoch number\n",
    "        for metric in history.history.keys():\n",
    "            row.append(history.history[metric][epoch])\n",
    "        data.append(row)\n",
    "\n",
    "    # Create headers\n",
    "    headers = ['epoch'] + list(history.history.keys())\n",
    "\n",
    "    # Print table\n",
    "    print(tabulate(data, headers=headers, floatfmt='.4f', tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 4] - Data Processing Functions [data_processor.py]\n",
    "def load_and_split_data(data_dir, split_index=0):\n",
    "    images_by_class = {}\n",
    "    labels_by_class = {}\n",
    "\n",
    "    class_folders = sorted(os.listdir(data_dir))\n",
    "    for class_index, class_folder in enumerate(class_folders):\n",
    "        print(f'> [load_and_split_data] Data preparing from the class {class_index} {class_folder}')\n",
    "        class_path = os.path.join(data_dir, class_folder)\n",
    "        if os.path.isdir(class_path):\n",
    "            img_files = sorted(os.listdir(class_path))\n",
    "            \n",
    "            images_by_class[class_index] = []\n",
    "            labels_by_class[class_index] = []\n",
    "            \n",
    "            for img_file in img_files:\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                image = load_img(\n",
    "                    img_path,\n",
    "                    target_size=hyperparameters['img_size'],\n",
    "                    color_mode=hyperparameters['color_mode']\n",
    "                )\n",
    "                image = img_to_array(image) / 255.0\n",
    "                images_by_class[class_index].append(image)\n",
    "                labels_by_class[class_index].append(class_index)\n",
    "\n",
    "    train_images, train_labels = [], []\n",
    "    test_images, test_labels = [], []\n",
    "\n",
    "    print(\"\\n=== [DATA DISTRIBUTION] ===\")\n",
    "    print(f\"{'Class':^10} {'Total':^10} {'Train':^10} {'Test':^10} {'Start':^10} {'End':^10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    test_size = hyperparameters['test_split_size']\n",
    "    for class_idx in sorted(images_by_class.keys()):\n",
    "        X = np.array(images_by_class[class_idx])\n",
    "        y = np.array(labels_by_class[class_idx])\n",
    "        \n",
    "        total_samples = len(X)\n",
    "        chunk_size = int(total_samples * test_size)\n",
    "        start_idx = split_index * chunk_size\n",
    "        end_idx = start_idx + chunk_size\n",
    "\n",
    "        test_mask = np.zeros(total_samples, dtype=bool)\n",
    "        test_mask[start_idx:end_idx] = True\n",
    "        \n",
    "        test_images.extend(X[test_mask])\n",
    "        test_labels.extend(y[test_mask])\n",
    "        train_images.extend(X[~test_mask])\n",
    "        train_labels.extend(y[~test_mask])\n",
    "\n",
    "        print(f\"{class_idx:^10} {total_samples:^10} {sum(~test_mask):^10} {sum(test_mask):^10} {start_idx:^10} {end_idx:^10}\")\n",
    "\n",
    "    return np.array(train_images), np.array(train_labels), np.array(test_images), np.array(test_labels)\n",
    "\n",
    "def save_processed_data(data_dir, split_index):\n",
    "    base_name = f\"{data_dir.split('/')[-2]}_{data_dir.split('/')[-1]}_split_{split_index}\"\n",
    "    save_path = os.path.join(DATA_DIR, base_name)\n",
    "\n",
    "    if os.path.exists(f\"{save_path}_train.pkl\"):\n",
    "        print(f\"> Data already saved in {save_path} (_train.pkl or _test.pkl)\")\n",
    "        return\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = load_and_split_data(data_dir, split_index)\n",
    "\n",
    "    with open(f\"{save_path}_train.pkl\", 'wb') as f:\n",
    "        pickle.dump((x_train, y_train), f)\n",
    "    with open(f\"{save_path}_test.pkl\", 'wb') as f:\n",
    "        pickle.dump((x_test, y_test), f)\n",
    "    \n",
    "    print(f\"> Saved processed data to {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "def load_processed_data(base_path):\n",
    "    with open(f\"{base_path}_train.pkl\", 'rb') as f:\n",
    "        x_train, y_train = pickle.load(f)\n",
    "    with open(f\"{base_path}_test.pkl\", 'rb') as f:\n",
    "        x_test, y_test = pickle.load(f)\n",
    "\n",
    "    test_size = hyperparameters['test_split_size']\n",
    "    print(\"=== Summary of Loaded Data: ===\")\n",
    "    print(f\"[DATAINFO] Total images: {len(x_train) + len(x_test)}\")\n",
    "    print(f\"[DATAINFO] Training set: {len(x_train)} images ({(1-test_size)*100:.0f}%)\")\n",
    "    print(f\"[DATAINFO] Test set: {len(x_test)} images ({test_size*100:.0f}%)\")\n",
    "    print(f\"[DATAINFO] Image shape: {x_train[0].shape}\")\n",
    "    print()\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to build U-Net (custom   implementation)\n",
    "def build_unet_model(input_shape=(128, 128, 3)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoding (down-sampling) path\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "    # Bridge (bottom of the U-Net)\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    \n",
    "    # Decoding (up-sampling) path\n",
    "    u1 = layers.UpSampling2D((2, 2))(c3)\n",
    "    u1 = layers.Concatenate()([u1, c2])\n",
    "    \n",
    "    u2 = layers.UpSampling2D((2, 2))(u1)\n",
    "    u2 = layers.Concatenate()([u2, c1])\n",
    "    \n",
    "    outputs = layers.Conv2D(3, (1, 1), activation='softmax')(u2)\n",
    "    \n",
    "    return models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 6] - Model Building Functions\n",
    "def build_custom_cnn_model(input_shape=(128, 128, 1), num_classes=4, architecture_name='cnn', fine_tune=True):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    if architecture_name == 'cnn':\n",
    "        # Use the values from the hyperparameters dictionary\n",
    "        x = layers.Conv2D(hyperparameters['cnn_filters'][0], hyperparameters['cnn_kernel_size'], \n",
    "                          activation=hyperparameters['cnn_activation_function'], padding=hyperparameters['cnn_padding'])(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D(hyperparameters['cnn_pool_size'])(x)\n",
    "        \n",
    "        x = layers.Conv2D(hyperparameters['cnn_filters'][1], hyperparameters['cnn_kernel_size'], \n",
    "                          activation=hyperparameters['cnn_activation_function'], padding=hyperparameters['cnn_padding'])(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D(hyperparameters['cnn_pool_size'])(x)\n",
    "        \n",
    "        x = layers.Conv2D(hyperparameters['cnn_filters'][2], hyperparameters['cnn_kernel_size'], \n",
    "                          activation=hyperparameters['cnn_activation_function'], padding=hyperparameters['cnn_padding'])(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D(hyperparameters['cnn_pool_size'])(x)\n",
    "        \n",
    "        x = layers.Conv2D(hyperparameters['cnn_filters'][3], hyperparameters['cnn_kernel_size'], \n",
    "                          activation=hyperparameters['cnn_activation_function'], padding=hyperparameters['cnn_padding'])(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "        x = layers.Dense(hyperparameters['cnn_dense_units'], activation=hyperparameters['cnn_activation_function'])(x)\n",
    "        x = layers.Dropout(hyperparameters['cnn_dropout_rate'])(x)\n",
    "    else:\n",
    "        # Upsample input to the required size if needed (InceptionV3 requires minimum 75x75)\n",
    "        if architecture_name == 'inceptionv3' and (input_shape[0] < 75 or input_shape[1] < 75):\n",
    "            required_size = (75, 75)  # InceptionV3 minimum size\n",
    "        else:\n",
    "            required_size = (input_shape[0], input_shape[1])  # Default size for other models\n",
    "            \n",
    "        x = layers.Resizing(required_size[0], required_size[1])(inputs)  # Resize input to required size\n",
    "\n",
    "        # Convert grayscale (1-channel) to 3-channel RGB for pretrained models\n",
    "        x = layers.Conv2D(3, (1, 1))(x)\n",
    "        \n",
    "        # Pretrained model selection logic\n",
    "        # ResNet Variants\n",
    "        if architecture_name == 'resnet50':\n",
    "            from tensorflow.keras.applications import ResNet50\n",
    "            base_model = ResNet50(include_top=False, weights=hyperparameters['pretained_model_weights'])\n",
    "        elif architecture_name == 'resnet101':\n",
    "            from tensorflow.keras.applications import ResNet101\n",
    "            base_model = ResNet101(include_top=False, weights=hyperparameters['pretained_model_weights'])\n",
    "        elif architecture_name == 'resnet152':\n",
    "            from tensorflow.keras.applications import ResNet152\n",
    "            base_model = ResNet152(include_top=False, weights=hyperparameters['pretained_model_weights'])\n",
    "        \n",
    "        # EfficientNetB0\n",
    "        elif architecture_name == 'efficientnetb0':\n",
    "            from tensorflow.keras.applications import EfficientNetB0\n",
    "            base_model = EfficientNetB0(include_top=False, weights=hyperparameters['pretained_model_weights'])\n",
    "\n",
    "        # VGGNet Variants\n",
    "        elif architecture_name == 'vgg16':\n",
    "            from tensorflow.keras.applications import VGG16\n",
    "            base_model = VGG16(include_top=False, weights=hyperparameters['pretained_model_weights'])\n",
    "        elif architecture_name == 'vgg19':\n",
    "            from tensorflow.keras.applications import VGG19\n",
    "            base_model = VGG19(include_top=False, weights=hyperparameters['pretained_model_weights'])\n",
    "\n",
    "        # InceptionV3\n",
    "        elif architecture_name == 'inceptionv3':\n",
    "            from tensorflow.keras.applications import InceptionV3\n",
    "            base_model = InceptionV3(include_top=False, weights=hyperparameters['pretained_model_weights'])\n",
    "\n",
    "        # U-Net (not from Keras applications, custom U-Net function)\n",
    "        elif architecture_name == 'unet':\n",
    "            base_model = build_unet_model(input_shape=(required_size[0], required_size[1], 3))  # Custom function to build U-Net model\n",
    "\n",
    "        # Set base model to non-trainable if fine-tuning is disabled\n",
    "        if not fine_tune:\n",
    "            base_model.trainable = False\n",
    "        else:\n",
    "            base_model.trainable = True\n",
    "        \n",
    "        # Apply base model to input\n",
    "        x = base_model(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile the model using learning rate and loss from the hyperparameters\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparameters['learning_rate']),\n",
    "        loss= hyperparameters['cnn_loss_function'],\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 7] - Pretext Task Functions\n",
    "def augment_image(image, rotation_angle):\n",
    "    if rotation_angle == 90:\n",
    "        image = tf.image.rot90(image)\n",
    "    elif rotation_angle == 180:\n",
    "        image = tf.image.rot90(image, k=2)\n",
    "    elif rotation_angle == 270:\n",
    "        image = tf.image.rot90(image, k=3)\n",
    "    \n",
    "    label = rotation_angle // 90\n",
    "    return image, label\n",
    "\n",
    "def preprocess_data(images):\n",
    "    augmented_images = []\n",
    "    labels = []\n",
    "    for image in images:\n",
    "        for rotation_angle in [0, 90, 180, 270]:\n",
    "            aug_image, label = augment_image(image, rotation_angle)\n",
    "            augmented_images.append(aug_image)\n",
    "            labels.append(label)\n",
    "    print(f'> {len(augmented_images)} augmented images generated each of shape {augmented_images[0].shape} with {len(labels)} labels\\n')\n",
    "    return np.array(augmented_images), np.array(labels)\n",
    "\n",
    "def save_model(model, model_path):\n",
    "    try:\n",
    "        try:\n",
    "            model.save(model_path, save_format='h5')\n",
    "        except Exception as h5_error:\n",
    "            print(f\"> H5 saving failed, trying SavedModel format: {h5_error}\")\n",
    "            model_path = model_path.replace('.h5', '')\n",
    "            model.save(model_path, save_format='tf')\n",
    "            \n",
    "        print(f\"> Model successfully saved at: {model_path}\\n\")\n",
    "        return model_path\n",
    "    except Exception as e:\n",
    "        print(f\"> Error saving model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_model(architecture_name, data_path, split_index):\n",
    "    model_name = f\"pretext_model_{os.path.basename(data_path)}_{architecture_name}.h5\"\n",
    "    model_path = os.path.join(MODEL_DIR, model_name)\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(model_path):\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "        else:\n",
    "            model_path = model_path.replace('.h5', '')\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "        print(f\"> Model successfully loaded from {model_path}\\n\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"> Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def run_pretext_pipeline(x_augmented, y_augmented, architecture_name, model_path):    \n",
    "    model = build_custom_cnn_model(\n",
    "        input_shape=hyperparameters['input_shape'],\n",
    "        num_classes=hyperparameters['num_classes'],\n",
    "        architecture_name=architecture_name,\n",
    "        fine_tune=hyperparameters['fine_tune']\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=hyperparameters['early_stopping_patience'],\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=hyperparameters['lr_reduction_factor'],\n",
    "            patience=hyperparameters['lr_reduction_patience']\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    '''Not proceeding with making a static validation set as not quite sure about the stratified nature'''\n",
    "    history = model.fit(\n",
    "        x_augmented, y_augmented,\n",
    "        validation_split = hyperparameters['validation_split'],\n",
    "        batch_size=hyperparameters['batch_size'],\n",
    "        epochs=hyperparameters['epochs'],\n",
    "        callbacks=callbacks,\n",
    "        shuffle=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Save the trained model\n",
    "    save_model(model, model_path)\n",
    "    \n",
    "    return history\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 8] - Downstream Task Functions\n",
    "def extract_features(pretext_model, x_data, layer_index=-2):\n",
    "    intermediate_model = models.Model(inputs=pretext_model.input, outputs=pretext_model.layers[layer_index].output)\n",
    "    features = intermediate_model.predict(x_data, verbose=2)\n",
    "    print(f'> extracted features of shape {features.shape}')\n",
    "    return features\n",
    "\n",
    "def evaluate_downstream_task(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average='macro'),\n",
    "        'recall': recall_score(y_test, y_pred, average='macro'),\n",
    "        'f1': f1_score(y_test, y_pred, average='macro'),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "def train_downstream_task(train_features, train_labels, test_features, test_labels, classifier='random_forest', n_splits=None):\n",
    "    # Assign number of splits from hyperparameters if not specified\n",
    "    if n_splits is None:\n",
    "        n_splits = hyperparameters['cv_splits']\n",
    "    \n",
    "    # Initialize classifier based on hyperparameters\n",
    "    if classifier == 'random_forest':\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=hyperparameters['rf_n_estimators'],\n",
    "            random_state=RANDOM_SEED,\n",
    "            verbose=0\n",
    "        )\n",
    "    elif classifier == 'svm':\n",
    "        clf = SVC(\n",
    "            kernel=hyperparameters['svm_kernel'],\n",
    "            random_state=RANDOM_SEED,\n",
    "            verbose=False\n",
    "        )\n",
    "    elif classifier == 'gradient_boosting':\n",
    "        clf = GradientBoostingClassifier(\n",
    "            n_estimators=hyperparameters['gb_n_estimators'],\n",
    "            random_state=RANDOM_SEED,\n",
    "            verbose=0\n",
    "        )\n",
    "    elif classifier == 'xgboost':\n",
    "        clf = XGBClassifier(\n",
    "            n_estimators=hyperparameters['xgb_n_estimators'],\n",
    "            random_state=RANDOM_SEED,\n",
    "            use_label_encoder=False,  # Update for recent versions of XGBoost\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    cv_scores = {\n",
    "        'accuracies': [], 'precisions': [], 'recalls': [], 'f1_scores': []\n",
    "    }\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(train_features, train_labels):\n",
    "        X_train_fold, X_val_fold = train_features[train_idx], train_features[val_idx]\n",
    "        y_train_fold, y_val_fold = train_labels[train_idx], train_labels[val_idx]\n",
    "        \n",
    "        clf.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = clf.predict(X_val_fold)\n",
    "        \n",
    "        # Append cross-validation scores\n",
    "        cv_scores['accuracies'].append(accuracy_score(y_val_fold, y_pred))\n",
    "        cv_scores['precisions'].append(precision_score(y_val_fold, y_pred, average='macro'))\n",
    "        cv_scores['recalls'].append(recall_score(y_val_fold, y_pred, average='macro'))\n",
    "        cv_scores['f1_scores'].append(f1_score(y_val_fold, y_pred, average='macro'))\n",
    "    \n",
    "    # Train final classifier on full training data and evaluate on test data\n",
    "    clf.fit(train_features, train_labels)\n",
    "    test_metrics = evaluate_downstream_task(clf, test_features, test_labels)\n",
    "    \n",
    "    return clf, cv_scores, test_metrics\n",
    "\n",
    "def run_downstream_pipeline(train_features, y_train, test_features, y_test, downstream_classifier):\n",
    "    clf, cv_scores, test_metrics = train_downstream_task(\n",
    "        train_features, y_train,\n",
    "        test_features, y_test,\n",
    "        classifier=downstream_classifier,\n",
    "        n_splits=hyperparameters['cv_splits']\n",
    "    )\n",
    "    \n",
    "    return clf, cv_scores, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 10] - Main Execution\n",
    "def main():\n",
    "    set_all_seeds(RANDOM_SEED)\n",
    "\n",
    "    for dir_path in [MODEL_DIR, DATA_DIR, RESULTS_DIR]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # Check for GPU\n",
    "    print(\"> GPU Availability: \", tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "    # Parameters\n",
    "    mode = experiment_parameters['mode']\n",
    "    split_index = experiment_parameters['split_index']\n",
    "    architecture = experiment_parameters['architecture']\n",
    "    classifier = experiment_parameters['classifier']\n",
    "    \n",
    "    # Data directory\n",
    "    data_dir = experiment_parameters['data_dir']\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Process and save data\n",
    "        if mode in ['process', 'all']:\n",
    "            print(\"\\n=== Processing Data ===\")\n",
    "            data_path = save_processed_data(data_dir, split_index)\n",
    "        \n",
    "        # Step 2: Train pretext model\n",
    "        if mode in ['pretext', 'all']:\n",
    "            print(\"\\n=== Training Pretext Model ===\")\n",
    "            data_path = os.path.join(DATA_DIR, f\"{data_dir.split('/')[-2]}_{data_dir.split('/')[-1]}_split_{split_index}\")\n",
    "            x_train, _, _, _ = load_processed_data(data_path)\n",
    "            x_augmented, y_augmented = preprocess_data(x_train)\n",
    "\n",
    "            architectures = architecture_list if architecture == 'all' else [architecture]\n",
    "            for arch in architectures:\n",
    "                model_name = f\"pretext_model_{os.path.basename(data_path)}_{arch}.h5\"\n",
    "                model_path = os.path.join(MODEL_DIR, model_name)\n",
    "\n",
    "                if os.path.exists(model_path):\n",
    "                    print(f\"> Alredy exists in {model_path}. No need for training.\\n\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\">> Begin training architecture: {arch}\")\n",
    "                history = run_pretext_pipeline(x_augmented, y_augmented, arch, model_path)\n",
    "                print_training_history(history)\n",
    "                # plot_training_history(history)\n",
    "                print(f\">> End training architecture: {arch}\\n\")\n",
    "\n",
    "            del x_train\n",
    "            del x_augmented\n",
    "            del y_augmented\n",
    "            gc.collect()\n",
    "        \n",
    "        # Step 3: Run downstream task\n",
    "        if mode in ['downstream', 'all']:\n",
    "            print(\"\\n=== Running Downstream Task ===\")\n",
    "            data_path = os.path.join(DATA_DIR, f\"{data_dir.split('/')[-2]}_{data_dir.split('/')[-1]}_split_{split_index}\")\n",
    "            x_train, y_train, x_test, y_test = load_processed_data(data_path)\n",
    "            \n",
    "            results_tracker = ResultsTracker()\n",
    "            architectures = architecture_list if architecture == 'all' else [architecture]\n",
    "            classifiers = classifier_list if classifier == 'all' else [classifier]\n",
    "            \n",
    "            for arch in architectures:\n",
    "                pretext_model = load_model(arch, data_path, split_index)\n",
    "                train_features = extract_features(pretext_model, x_train, hyperparameters['feature_extraction_layer'])\n",
    "                test_features = extract_features(pretext_model, x_test, hyperparameters['feature_extraction_layer'])\n",
    "                print()\n",
    "    \n",
    "                for clf in classifiers:\n",
    "                    print(f\">> Begin training and evaluating classifier: {clf} with architecture: {arch}\")\n",
    "                    classifier_model, cv_scores, test_metrics = run_downstream_pipeline(\n",
    "                        train_features, y_train, test_features, y_test, clf\n",
    "                    )\n",
    "                    \n",
    "                    results_tracker.add_result(\n",
    "                        data_dir=data_dir,\n",
    "                        architecture=arch,\n",
    "                        classifier=clf,\n",
    "                        cv_metrics=cv_scores,\n",
    "                        test_metrics=test_metrics\n",
    "                    )\n",
    "                    print(f\">> End training and evaluating classifier: {clf} with architecture: {arch}\\n\")\n",
    "\n",
    "                # results_tracker.display_results()\n",
    "                del pretext_model\n",
    "                del train_features\n",
    "                del test_features\n",
    "                gc.collect()\n",
    "\n",
    "            del x_train\n",
    "            del x_test\n",
    "            del y_train\n",
    "            del y_test\n",
    "            gc.collect()\n",
    "\n",
    "            results_tracker.display_results()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Output is being saved on exp-04/results/2024-11-01-14h-29m-48s-dir_qpm_real-mode_all-split_0-arch_all-clf_all.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 14:29:48.995701: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-11-01 14:29:48.995961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-Q7I3MQ5): /proc/driver/nvidia/version does not exist\n",
      "2024-11-01 14:29:49.788984: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# [Cell 11] - Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d-%Hh-%Mm-%Ss')\n",
    "    filename = os.path.join(RESULTS_DIR, f\"{timestamp}-dir_{experiment_parameters['data_dir'].split('/')[-2]}_{experiment_parameters['data_dir'].split('/')[-1]}-mode_{experiment_parameters['mode']}-split_{experiment_parameters['split_index']}-arch_{experiment_parameters['architecture']}-clf_{experiment_parameters['classifier']}.txt\")\n",
    "    print(f\"> Output is being saved on {filename}\")\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'w') as output_file:\n",
    "            sys.stdout = output_file\n",
    "            print(filename)\n",
    "            display_configurations()\n",
    "            main()\n",
    "            sys.stdout = sys.__stdout__\n",
    "    except Exception as e:\n",
    "        sys.stdout = sys.__stdout__\n",
    "        print(\"ERROR OCCURED\", e)\n",
    "        \n",
    "    print(\"                                        === PROCESS FINISHED ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
